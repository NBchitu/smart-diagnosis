"""
AIç½‘ç»œåˆ†ææœåŠ¡
è´Ÿè´£æ ¹æ®æŠ“åŒ…æ•°æ®ç”ŸæˆAIåˆ†ææŠ¥å‘Š
"""

import logging
import json
import os
import copy
from typing import Dict, Any, Optional, List
from datetime import datetime
from pathlib import Path

from app.config.ai_config import get_ai_config
import openai
import anthropic
import requests

logger = logging.getLogger(__name__)

# åˆ›å»ºè°ƒè¯•æ•°æ®ç›®å½•
DEBUG_DIR = Path('/tmp/ai_analysis_debug')
DEBUG_DIR.mkdir(exist_ok=True)

class AIAnalysisService:
    """AIåˆ†ææœåŠ¡"""
    
    def __init__(self):
        self.ai_config = get_ai_config()

    def _save_debug_data(self, task_id: str, issue_type: str, capture_summary: Dict,
                        user_description: str, prompt: str, ai_response: str = None) -> str:
        """ä¿å­˜è°ƒè¯•æ•°æ®åˆ°æ–‡ä»¶"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            debug_filename = f"ai_analysis_{timestamp}_{task_id[:8]}.json"
            debug_file_path = DEBUG_DIR / debug_filename

            debug_data = {
                'metadata': {
                    'task_id': task_id,
                    'timestamp': datetime.now().isoformat(),
                    'issue_type': issue_type,
                    'user_description': user_description,
                    'ai_provider': self.ai_config.current_provider,
                    'ai_model': self.ai_config.get_current_config().model if self.ai_config.get_current_config() else 'unknown'
                },
                'input_data': {
                    'capture_summary': capture_summary,
                    'prompt_length': len(prompt),
                    'prompt_content': prompt
                },
                'output_data': {
                    'ai_response': ai_response,
                    'ai_response_length': len(ai_response) if ai_response else 0
                }
            }

            with open(debug_file_path, 'w', encoding='utf-8') as f:
                json.dump(debug_data, f, indent=2, ensure_ascii=False)

            logger.info(f"è°ƒè¯•æ•°æ®å·²ä¿å­˜åˆ°: {debug_file_path}")
            return str(debug_file_path)

        except Exception as e:
            logger.error(f"ä¿å­˜è°ƒè¯•æ•°æ®å¤±è´¥: {str(e)}")
            return ""

    def _update_debug_data_with_response(self, debug_file_path: str, ai_response: str):
        """æ›´æ–°è°ƒè¯•æ•°æ®æ–‡ä»¶ï¼Œæ·»åŠ AIå“åº”"""
        try:
            if os.path.exists(debug_file_path):
                with open(debug_file_path, 'r', encoding='utf-8') as f:
                    debug_data = json.load(f)

                debug_data['output_data']['ai_response'] = ai_response
                debug_data['output_data']['ai_response_length'] = len(ai_response)
                debug_data['metadata']['response_received_at'] = datetime.now().isoformat()

                with open(debug_file_path, 'w', encoding='utf-8') as f:
                    json.dump(debug_data, f, indent=2, ensure_ascii=False)

                logger.info(f"è°ƒè¯•æ•°æ®å·²æ›´æ–°: {debug_file_path}")

        except Exception as e:
            logger.error(f"æ›´æ–°è°ƒè¯•æ•°æ®å¤±è´¥: {str(e)}")
    
    def analyze_network_issue_sync(self, issue_type: str, capture_summary: Dict,
                                 user_description: Optional[str] = None, task_id: str = None,
                                 filtered_domains: Optional[List[str]] = None,
                                 latency_filter: Optional[str] = None) -> Dict[str, Any]:
        """
        åŒæ­¥ç‰ˆæœ¬çš„ç½‘ç»œé—®é¢˜åˆ†æï¼Œé¿å…äº‹ä»¶å¾ªç¯å†²çª
        """
        ai_response = None
        debug_file_path = ""

        try:
            logger.info(f"å¼€å§‹åŒæ­¥AIåˆ†æï¼Œé—®é¢˜ç±»å‹: {issue_type}")

            # æ£€æŸ¥AIé…ç½®
            config = self.ai_config.get_current_config()
            if not config:
                raise Exception("AIé…ç½®æ— æ•ˆæˆ–æœªè®¾ç½®")

            logger.info(f"ä½¿ç”¨AIæä¾›å•†: {self.ai_config.current_provider}")

            # ç”Ÿæˆåˆ†æprompt
            prompt = self._generate_analysis_prompt(issue_type, capture_summary, user_description,
                                                  filtered_domains, latency_filter)
            logger.info(f"ç”Ÿæˆprompté•¿åº¦: {len(prompt)} å­—ç¬¦")

            # ä¿å­˜è°ƒè¯•æ•°æ®ï¼ˆè¾“å…¥éƒ¨åˆ†ï¼‰
            if task_id:
                debug_file_path = self._save_debug_data(
                    task_id, issue_type, capture_summary,
                    user_description or "", prompt
                )

            # è°ƒç”¨AI APIï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰
            logger.info("å¼€å§‹è°ƒç”¨AI API")
            ai_response = self._call_ai_api_sync(prompt)
            logger.info(f"AI APIå“åº”é•¿åº¦: {len(ai_response)} å­—ç¬¦")

            # æ›´æ–°è°ƒè¯•æ•°æ®ï¼ˆè¾“å‡ºéƒ¨åˆ†ï¼‰
            if task_id and debug_file_path:
                self._update_debug_data_with_response(debug_file_path, ai_response)

            # è§£æAIå“åº”
            logger.info("å¼€å§‹è§£æAIå“åº”")
            analysis_result = self._parse_ai_response(ai_response, issue_type)

            logger.info("AIåˆ†ææˆåŠŸå®Œæˆ")
            result = {
                'success': True,
                'analysis': analysis_result,
                'timestamp': datetime.now().isoformat(),
                'ai_provider': self.ai_config.current_provider
            }

            # æ·»åŠ è°ƒè¯•æ–‡ä»¶è·¯å¾„åˆ°ç»“æœä¸­
            if debug_file_path:
                result['debug_file'] = debug_file_path

            return result

        except Exception as e:
            logger.error(f"AIåˆ†æå¤±è´¥: {str(e)}", exc_info=True)

            # å³ä½¿å¤±è´¥ä¹Ÿä¿å­˜è°ƒè¯•æ•°æ®
            if task_id and ai_response is None:
                try:
                    prompt = self._generate_analysis_prompt(issue_type, capture_summary, user_description,
                                                          filtered_domains, latency_filter)
                    debug_file_path = self._save_debug_data(
                        task_id, issue_type, capture_summary,
                        user_description or "", prompt, f"ERROR: {str(e)}"
                    )
                except:
                    pass

            result = {
                'success': False,
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }

            if debug_file_path:
                result['debug_file'] = debug_file_path

            return result

    async def analyze_network_issue(self, issue_type: str, capture_summary: Dict,
                                  user_description: Optional[str] = None,
                                   filtered_domains: Optional[List[str]] = None,
                                   latency_filter: Optional[str] = None) -> Dict[str, Any]:
        """
        åˆ†æç½‘ç»œé—®é¢˜

        Args:
            issue_type: é—®é¢˜ç±»å‹ (website_access, interconnection, game_lag, custom)
            capture_summary: æŠ“åŒ…æ•°æ®æ‘˜è¦
            user_description: ç”¨æˆ·æè¿°çš„é—®é¢˜
            filtered_domains: ç­›é€‰çš„åŸŸååˆ—è¡¨ï¼Œåªåˆ†æè¿™äº›åŸŸå
            latency_filter: é€Ÿåº¦åˆ†ç±»ç­›é€‰ (all/fast/slow/error)

        Returns:
            AIåˆ†æç»“æœ
        """
        try:
            logger.info(f"å¼€å§‹AIåˆ†æï¼Œé—®é¢˜ç±»å‹: {issue_type}")
            if filtered_domains:
                logger.info(f"ç­›é€‰åŸŸå: {filtered_domains}")
            if latency_filter and latency_filter != 'all':
                logger.info(f"é€Ÿåº¦åˆ†ç±»ç­›é€‰: {latency_filter}")

            # ç”Ÿæˆåˆ†æprompt
            prompt = self._generate_analysis_prompt(issue_type, capture_summary, user_description,
                                                  filtered_domains, latency_filter)
            logger.info(f"ç”Ÿæˆçš„prompté•¿åº¦: {len(prompt)} å­—ç¬¦")

            # ä½¿ç”¨åŒæ­¥æ–¹æ³•è°ƒç”¨AI APIï¼ˆé¿å…å¼‚æ­¥é—®é¢˜ï¼‰
            ai_response = self._call_ai_api_sync(prompt)
            logger.info(f"AIå“åº”é•¿åº¦: {len(ai_response)} å­—ç¬¦")

            # è§£æAIå“åº”
            analysis_result = self._parse_ai_response(ai_response, issue_type)
            logger.info("AIå“åº”è§£ææˆåŠŸ")

            # ä¿å­˜è°ƒè¯•æ•°æ®ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            try:
                debug_file_path = self._save_debug_data(
                    "ai_analysis", issue_type, capture_summary,
                    user_description or "", prompt, ai_response
                )
                if debug_file_path:
                    logger.info(f"è°ƒè¯•æ•°æ®å·²ä¿å­˜åˆ°: {debug_file_path}")
            except Exception as debug_e:
                logger.warning(f"ä¿å­˜è°ƒè¯•æ•°æ®å¤±è´¥: {str(debug_e)}")

            return {
                'success': True,
                'analysis': analysis_result,
                'timestamp': datetime.now().isoformat(),
                'ai_provider': self.ai_config.current_provider
            }

        except Exception as e:
            logger.error(f"AIåˆ†æå¤±è´¥: {str(e)}", exc_info=True)
            return {
                'success': False,
                'error': str(e),
                'timestamp': datetime.now().isoformat(),
                'analysis': {
                    'diagnosis': f'AIåˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}',
                    'severity': 'unknown',
                    'root_cause': 'æœåŠ¡å¼‚å¸¸æˆ–é…ç½®é—®é¢˜',
                    'recommendations': ['æ£€æŸ¥AIæœåŠ¡çŠ¶æ€', 'éªŒè¯é…ç½®ä¿¡æ¯', 'é‡è¯•åˆ†æ'],
                    'technical_details': f'é”™è¯¯è¯¦æƒ…: {str(e)}',
                    'confidence': 0
                }
            }
    
    def _generate_analysis_prompt(self, issue_type: str, capture_summary: Dict,
                                user_description: Optional[str] = None,
                                filtered_domains: Optional[List[str]] = None,
                                latency_filter: Optional[str] = None) -> str:
        """ç”ŸæˆAIåˆ†æprompt"""
        
        # åŸºç¡€promptæ¨¡æ¿
        base_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç½‘ç»œè¯Šæ–­ä¸“å®¶ï¼Œæ‹¥æœ‰ä¸°å¯Œçš„ç½‘ç»œæ•…éšœæ’æŸ¥ç»éªŒã€‚è¯·æ ¹æ®ä»¥ä¸‹ç½‘ç»œæŠ“åŒ…æ•°æ®è¿›è¡Œæ·±åº¦åˆ†æï¼Œè¯†åˆ«é—®é¢˜æ ¹æºå¹¶æä¾›å¯æ“ä½œçš„è§£å†³æ–¹æ¡ˆã€‚

åˆ†æè¦æ±‚ï¼š
1. ä»”ç»†åˆ†ææŠ“åŒ…æ•°æ®ä¸­çš„å…³é”®æŒ‡æ ‡
2. è¯†åˆ«å¼‚å¸¸æ¨¡å¼å’Œæ€§èƒ½ç“¶é¢ˆ
3. æä¾›å…·ä½“çš„é—®é¢˜å®šä½å’Œè§£å†³æ­¥éª¤
4. ç»™å‡ºä¼˜åŒ–å»ºè®®å’Œé¢„é˜²æªæ–½

è¯·æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼å›å¤ï¼š"""

        # åˆå§‹åŒ–prompt
        prompt = base_prompt

        # æ£€æŸ¥æ˜¯å¦æœ‰ç­›é€‰æ¡ä»¶
        has_domain_filter = filtered_domains and len(filtered_domains) > 0
        has_latency_filter = latency_filter and latency_filter != 'all'

        # æ ¹æ®æ˜¯å¦æœ‰ç­›é€‰æ¡ä»¶åŠ¨æ€ç”ŸæˆJSONæ ¼å¼æ¨¡æ¿
        if has_domain_filter or has_latency_filter:
            json_template = """{
    "diagnosis": "ç®€æ´æ˜ç¡®çš„é—®é¢˜è¯Šæ–­ç»“è®ºï¼ˆä»…é’ˆå¯¹ç­›é€‰çš„åŸŸåï¼‰",
    "severity": "ä¸¥é‡ç¨‹åº¦(low/medium/high/critical)",
    "root_cause": "è¯¦ç»†çš„æ ¹æœ¬åŸå› åˆ†æï¼ŒåŒ…æ‹¬æŠ€æœ¯ç»†èŠ‚ï¼ˆåŸºäºç­›é€‰åçš„æ•°æ®ï¼‰",
    "key_findings": [
        "å…³é”®å‘ç°1ï¼šå…·ä½“çš„æ•°æ®å¼‚å¸¸ï¼ˆä»…æ¶‰åŠç­›é€‰åŸŸåï¼‰",
        "å…³é”®å‘ç°2ï¼šæ€§èƒ½æŒ‡æ ‡é—®é¢˜ï¼ˆä»…æ¶‰åŠç­›é€‰åŸŸåï¼‰"
    ],
    "recommendations": [
        "ç«‹å³è¡ŒåŠ¨ï¼šç´§æ€¥è§£å†³æ­¥éª¤ï¼ˆé’ˆå¯¹ç­›é€‰çš„åŸŸåï¼‰",
        "çŸ­æœŸä¼˜åŒ–ï¼šå…·ä½“çš„é…ç½®è°ƒæ•´ï¼ˆé’ˆå¯¹ç­›é€‰çš„åŸŸåï¼‰",
        "é•¿æœŸæ”¹è¿›ï¼šç³»ç»Ÿæ€§ä¼˜åŒ–å»ºè®®"
    ],
    "diagnostic_clues": [
        "ğŸ“Š å…·ä½“çš„æ€§èƒ½æ•°æ®åˆ†æï¼ˆä»…ç­›é€‰åŸŸåçš„æ•°æ®ï¼‰",
        "ğŸ” å¼‚å¸¸æµé‡æ¨¡å¼å‘ç°ï¼ˆä»…ç­›é€‰åŸŸåçš„æµé‡ï¼‰",
        "âš ï¸ æ½œåœ¨é£é™©ç‚¹è¯†åˆ«ï¼ˆä»…ç­›é€‰åŸŸåç›¸å…³ï¼‰"
    ],
    "technical_details": "æ·±å…¥çš„æŠ€æœ¯åˆ†æå’Œæ•°æ®è§£è¯»ï¼ˆä¸¥æ ¼é™åˆ¶åœ¨ç­›é€‰åŸŸåèŒƒå›´å†…ï¼‰",
    "confidence": "è¯Šæ–­ç½®ä¿¡åº¦(0-100)",
    "next_steps": "å»ºè®®çš„åç»­ç›‘æ§å’ŒéªŒè¯æ­¥éª¤ï¼ˆé’ˆå¯¹ç­›é€‰çš„åŸŸåï¼‰"
}"""
        else:
            json_template = """{
    "diagnosis": "ç®€æ´æ˜ç¡®çš„é—®é¢˜è¯Šæ–­ç»“è®º",
    "severity": "ä¸¥é‡ç¨‹åº¦(low/medium/high/critical)",
    "root_cause": "è¯¦ç»†çš„æ ¹æœ¬åŸå› åˆ†æï¼ŒåŒ…æ‹¬æŠ€æœ¯ç»†èŠ‚",
    "key_findings": [
        "å…³é”®å‘ç°1ï¼šå…·ä½“çš„æ•°æ®å¼‚å¸¸",
        "å…³é”®å‘ç°2ï¼šæ€§èƒ½æŒ‡æ ‡é—®é¢˜"
    ],
    "recommendations": [
        "ç«‹å³è¡ŒåŠ¨ï¼šç´§æ€¥è§£å†³æ­¥éª¤",
        "çŸ­æœŸä¼˜åŒ–ï¼šå…·ä½“çš„é…ç½®è°ƒæ•´",
        "é•¿æœŸæ”¹è¿›ï¼šç³»ç»Ÿæ€§ä¼˜åŒ–å»ºè®®"
    ],
    "diagnostic_clues": [
        "ğŸ“Š å…·ä½“çš„æ€§èƒ½æ•°æ®åˆ†æ",
        "ğŸ” å¼‚å¸¸æµé‡æ¨¡å¼å‘ç°",
        "âš ï¸ æ½œåœ¨é£é™©ç‚¹è¯†åˆ«"
    ],
    "technical_details": "æ·±å…¥çš„æŠ€æœ¯åˆ†æå’Œæ•°æ®è§£è¯»",
    "confidence": "è¯Šæ–­ç½®ä¿¡åº¦(0-100)",
    "next_steps": "å»ºè®®çš„åç»­ç›‘æ§å’ŒéªŒè¯æ­¥éª¤"
}"""

        prompt += json_template

        # é‡æ–°è®¾è®¡çš„é—®é¢˜ç±»å‹ç‰¹å®šæŒ‡å¯¼ - ä¸“æ³¨äºä¸‰å¤§æ ¸å¿ƒåŠŸèƒ½
        issue_specific_prompts = {
            'website_access': """
ğŸŒ é—®é¢˜ç±»å‹ï¼šç½‘ç«™è®¿é—®é—®é¢˜
æ ¸å¿ƒè¯Šæ–­é‡ç‚¹ï¼š
- HTTP/HTTPSå“åº”æ—¶é—´åˆ†æï¼šæ­£å¸¸<500msï¼Œæ…¢>2sï¼Œè¶…æ—¶>10s
- TCPè¿æ¥å»ºç«‹æ—¶é—´ï¼šæ­£å¸¸<100msï¼Œæ…¢>500ms
- DNSè§£ææ—¶é—´ï¼šæ­£å¸¸<50msï¼Œæ…¢>200ms
- HTTPçŠ¶æ€ç åˆ†æï¼š2xxæˆåŠŸç‡åº”>95%
- ç½‘ç«™æœåŠ¡å™¨IPå½’å±å’ŒCDNåˆ†æ

å…·ä½“åˆ†ææŒ‡æ ‡ï¼š
1. è®¡ç®—å„ç½‘ç«™çš„å¹³å‡å“åº”æ—¶é—´å’ŒæˆåŠŸç‡
2. åˆ†æTCPä¸‰æ¬¡æ¡æ‰‹è€—æ—¶
3. æ£€æŸ¥HTTPé”™è¯¯çŠ¶æ€ç åˆ†å¸ƒ
4. è¯„ä¼°ä¸åŒç½‘ç«™çš„è®¿é—®è´¨é‡å·®å¼‚
5. è¯†åˆ«CDNèŠ‚ç‚¹é€‰æ‹©æ˜¯å¦åˆç†

ç½‘ç«™è®¿é—®æ€§èƒ½è¯„ä¼°æ ‡å‡†ï¼š
- ğŸ“Š ä¼˜ç§€ï¼šHTTPå“åº”<300msï¼ŒæˆåŠŸç‡>98%
- ğŸ“Š è‰¯å¥½ï¼šHTTPå“åº”300-800msï¼ŒæˆåŠŸç‡95-98%
- ğŸ“Š ä¸€èˆ¬ï¼šHTTPå“åº”800-2000msï¼ŒæˆåŠŸç‡90-95%
- ğŸ“Š è¾ƒå·®ï¼šHTTPå“åº”>2000msï¼ŒæˆåŠŸç‡<90%

å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆï¼š
- DNSè§£ææ…¢ â†’ æ›´æ¢DNSæœåŠ¡å™¨(å¦‚8.8.8.8)
- CDNèŠ‚ç‚¹è¿œ â†’ è”ç³»ç½‘ç«™æ–¹ä¼˜åŒ–CDNé…ç½®
- è¿è¥å•†çº¿è·¯é—®é¢˜ â†’ å°è¯•ä½¿ç”¨VPNæˆ–ä»£ç†
- æœ¬åœ°ç½‘ç»œæ‹¥å¡ â†’ æ£€æŸ¥å¸¦å®½ä½¿ç”¨æƒ…å†µ
""",
            'interconnection': """
ï¿½ é—®é¢˜ç±»å‹ï¼šäº’è”äº’é€šè®¿é—®
æ ¸å¿ƒè¯Šæ–­é‡ç‚¹ï¼š
- è·¨è¿è¥å•†è®¿é—®å»¶è¿Ÿï¼šç§»åŠ¨â†”è”é€šâ†”ç”µä¿¡
- ä¸åŒISPæœåŠ¡å™¨çš„è®¿é—®è´¨é‡å¯¹æ¯”
- äº’è”äº’é€šèŠ‚ç‚¹çš„æ€§èƒ½ç“¶é¢ˆè¯†åˆ«
- è·¯ç”±è·¯å¾„ä¼˜åŒ–åˆ†æ

å…·ä½“åˆ†ææŒ‡æ ‡ï¼š
1. è¯†åˆ«ç›®æ ‡æœåŠ¡å™¨çš„ISPå½’å±
2. è®¡ç®—è·¨è¿è¥å•†è®¿é—®çš„RTTå»¶è¿Ÿ
3. åˆ†æä¸åŒè¿è¥å•†çº¿è·¯çš„ä¸¢åŒ…ç‡
4. è¯„ä¼°äº’è”äº’é€šè´¨é‡ç­‰çº§
5. æ£€æµ‹è·¯ç”±ç»•è¡Œå’Œæ¬¡ä¼˜è·¯å¾„

äº’è”äº’é€šè´¨é‡è¯„ä¼°ï¼š
- ğŸ“Š ä¼˜ç§€ï¼šè·¨ç½‘å»¶è¿Ÿ<50msï¼Œä¸¢åŒ…ç‡<0.1%
- ğŸ“Š è‰¯å¥½ï¼šè·¨ç½‘å»¶è¿Ÿ50-100msï¼Œä¸¢åŒ…ç‡0.1-0.5%
- ğŸ“Š ä¸€èˆ¬ï¼šè·¨ç½‘å»¶è¿Ÿ100-200msï¼Œä¸¢åŒ…ç‡0.5-1%
- ğŸ“Š è¾ƒå·®ï¼šè·¨ç½‘å»¶è¿Ÿ>200msï¼Œä¸¢åŒ…ç‡>1%

è¿è¥å•†äº’è”åˆ†æï¼š
- ä¸­å›½ç§»åŠ¨ â†” ä¸­å›½è”é€šï¼šé‡ç‚¹å…³æ³¨åŒ—æ–¹åœ°åŒºäº’è”è´¨é‡
- ä¸­å›½ç§»åŠ¨ â†” ä¸­å›½ç”µä¿¡ï¼šé‡ç‚¹å…³æ³¨å—æ–¹åœ°åŒºäº’è”è´¨é‡
- æ•™è‚²ç½‘ â†” å•†ä¸šç½‘ï¼šå…³æ³¨å­¦æœ¯èµ„æºè®¿é—®è´¨é‡

ä¼˜åŒ–å»ºè®®ï¼š
- é€‰æ‹©åŒè¿è¥å•†æœåŠ¡å™¨ â†’ é¿å…è·¨ç½‘è®¿é—®
- ä½¿ç”¨CDNåŠ é€Ÿ â†’ å°±è¿‘è®¿é—®ä¼˜è´¨èŠ‚ç‚¹
- å¤šçº¿è·¯æ¥å…¥ â†’ BGPå¤šçº¿æˆ–åŒçº¿æ¥å…¥
- è”ç³»è¿è¥å•† â†’ åé¦ˆäº’è”è´¨é‡é—®é¢˜
""",
            'game_lag': """
ï¿½ é—®é¢˜ç±»å‹ï¼šæ¸¸æˆå¡é¡¿é—®é¢˜
æ ¸å¿ƒè¯Šæ–­é‡ç‚¹ï¼š
- æ¸¸æˆæµé‡è¯†åˆ«å’ŒæœåŠ¡å™¨IPå½’å±åˆ†æ
- æ¸¸æˆæœåŠ¡å™¨æ˜¯å¦ä¸ºä¸­å›½ç§»åŠ¨ç½‘ç»œ
- UDPåŒ…ä¸¢å¤±ç‡å’Œå»¶è¿ŸæŠ–åŠ¨åˆ†æ
- æ¸¸æˆä¸“ç”¨ç«¯å£çš„è¿æ¥è´¨é‡

æ¸¸æˆæµé‡è¯†åˆ«ç®—æ³•ï¼š
1. ç«¯å£ç‰¹å¾ï¼š7000-8100, 17500-17600, 10000-15000
2. åè®®ç‰¹å¾ï¼šUDPå æ¯”>60%ï¼ŒåŒ…å¤§å°50-800å­—èŠ‚
3. æµé‡æ¨¡å¼ï¼šé«˜é¢‘ç‡åŒå‘é€šä¿¡ï¼Œæœ‰è§„å¾‹å¿ƒè·³åŒ…
4. æ—¶åºç‰¹å¾ï¼š30-60fpså¯¹åº”çš„åŒ…å‘é€é¢‘ç‡

æ¸¸æˆæœåŠ¡å™¨ISPåˆ†æï¼š
- ğŸ¯ ä¸­å›½ç§»åŠ¨æ¸¸æˆæœåŠ¡å™¨ï¼šå»¶è¿Ÿ<30msï¼Œæœ€ä¼˜ä½“éªŒ
- ğŸ¯ ä¸­å›½è”é€šæ¸¸æˆæœåŠ¡å™¨ï¼šå»¶è¿Ÿ30-50msï¼Œè‰¯å¥½ä½“éªŒ
- ğŸ¯ ä¸­å›½ç”µä¿¡æ¸¸æˆæœåŠ¡å™¨ï¼šå»¶è¿Ÿ50-80msï¼Œä¸€èˆ¬ä½“éªŒ
- ğŸ¯ æµ·å¤–æ¸¸æˆæœåŠ¡å™¨ï¼šå»¶è¿Ÿ>100msï¼Œéœ€è¦ä¼˜åŒ–

æ¸¸æˆç½‘ç»œè´¨é‡è¯„ä¼°ï¼š
- ï¿½ ä¼˜ç§€ï¼šå»¶è¿Ÿ<30msï¼Œä¸¢åŒ…ç‡<0.01%ï¼ŒæŠ–åŠ¨<5ms
- ğŸ“Š è‰¯å¥½ï¼šå»¶è¿Ÿ30-50msï¼Œä¸¢åŒ…ç‡0.01-0.1%ï¼ŒæŠ–åŠ¨5-10ms
- ğŸ“Š ä¸€èˆ¬ï¼šå»¶è¿Ÿ50-80msï¼Œä¸¢åŒ…ç‡0.1-0.5%ï¼ŒæŠ–åŠ¨10-20ms
- ğŸ“Š è¾ƒå·®ï¼šå»¶è¿Ÿ>80msï¼Œä¸¢åŒ…ç‡>0.5%ï¼ŒæŠ–åŠ¨>20ms

æ¸¸æˆä¼˜åŒ–å»ºè®®ï¼š
- é€‰æ‹©ç§»åŠ¨æ¸¸æˆæœåŠ¡å™¨ â†’ è·å¾—æœ€ä½³ç½‘ç»œä½“éªŒ
- å…³é—­åå°åº”ç”¨ â†’ å‡å°‘ç½‘ç»œå¸¦å®½å ç”¨
- ä½¿ç”¨æ¸¸æˆåŠ é€Ÿå™¨ â†’ ä¼˜åŒ–è·¯ç”±è·¯å¾„
- å‡çº§ç½‘ç»œå¥—é¤ â†’ æé«˜å¸¦å®½å’Œä¼˜å…ˆçº§
- ä½¿ç”¨æœ‰çº¿è¿æ¥ â†’ é¿å…WiFiä¸ç¨³å®š
""",
            'custom': """
ğŸ”§ é—®é¢˜ç±»å‹ï¼šè‡ªå®šä¹‰ç½‘ç»œé—®é¢˜
æ ¸å¿ƒè¯Šæ–­é‡ç‚¹ï¼š
- å…¨é¢çš„ç½‘ç»œæ€§èƒ½åŸºçº¿åˆ†æ
- å¼‚å¸¸æµé‡æ¨¡å¼è¯†åˆ«
- åè®®åˆ†å¸ƒå’Œåº”ç”¨è¡Œä¸ºåˆ†æ
- å®‰å…¨å¨èƒå’Œå¼‚å¸¸è¿æ¥æ£€æµ‹

å…·ä½“åˆ†ææŒ‡æ ‡ï¼š
1. å»ºç«‹ç½‘ç»œæ€§èƒ½åŸºçº¿æŒ‡æ ‡
2. è¯†åˆ«å¼‚å¸¸çš„æµé‡æ¨¡å¼å’Œå³°å€¼
3. åˆ†æåº”ç”¨å±‚åè®®çš„ä½¿ç”¨æƒ…å†µ
4. æ£€æµ‹æ½œåœ¨çš„å®‰å…¨å¨èƒ
5. è¯„ä¼°æ•´ä½“ç½‘ç»œå¥åº·çŠ¶å†µ

é€šç”¨åˆ†ææ–¹æ³•ï¼š
- æµé‡åˆ†æ â†’ è¯†åˆ«å¼‚å¸¸åº”ç”¨å’Œç”¨æˆ·è¡Œä¸º
- æ€§èƒ½ç›‘æ§ â†’ å»ºç«‹å…³é”®æŒ‡æ ‡çš„ç›‘æ§ä½“ç³»
- å®‰å…¨æ£€æŸ¥ â†’ å‘ç°æ½œåœ¨çš„å®‰å…¨é£é™©
- å®¹é‡è§„åˆ’ â†’ é¢„æµ‹æœªæ¥çš„ç½‘ç»œéœ€æ±‚
"""
        }
        
        # æ„å»ºå®Œæ•´prompt
        prompt = base_prompt

        # æ·»åŠ é—®é¢˜ç±»å‹ç‰¹å®šçš„åˆ†ææŒ‡å¯¼
        if issue_type in issue_specific_prompts:
            prompt += issue_specific_prompts[issue_type]
        else:
            prompt += issue_specific_prompts.get('custom', '')

        # æ·»åŠ ç”¨æˆ·æè¿°
        if user_description:
            prompt += f"\nğŸ“ ç”¨æˆ·æè¿°çš„é—®é¢˜ï¼š{user_description}\n"

        # æ·»åŠ æŠ“åŒ…ç¯å¢ƒä¿¡æ¯
        prompt += "\nğŸ” æŠ“åŒ…ç¯å¢ƒä¿¡æ¯ï¼š\n"
        if capture_summary.get('statistics'):
            stats = capture_summary['statistics']
            prompt += f"- æŠ“åŒ…æ—¶é•¿: {stats.get('duration', 'N/A')}ç§’\n"
            prompt += f"- æ€»åŒ…æ•°: {stats.get('total_packets', 'N/A')}\n"
            prompt += f"- æ•°æ®é‡: {stats.get('total_bytes', 'N/A')}å­—èŠ‚\n"

        # æ·»åŠ å…³é”®æ€§èƒ½æŒ‡æ ‡
        if capture_summary.get('enhanced_analysis'):
            enhanced = capture_summary['enhanced_analysis']
            prompt += "\nğŸ“Š å…³é”®æ€§èƒ½æŒ‡æ ‡ï¼š\n"

            if enhanced.get('network_quality'):
                nq = enhanced['network_quality']
                prompt += f"- å¹³å‡RTT: {nq.get('avg_rtt', 'N/A')}ms\n"
                prompt += f"- ä¸¢åŒ…ç‡: {nq.get('packet_loss_rate', 0)*100:.2f}%\n"
                prompt += f"- é‡ä¼ æ¬¡æ•°: {nq.get('retransmissions', 'N/A')}\n"

            if enhanced.get('http_analysis'):
                http = enhanced['http_analysis']
                prompt += f"- HTTPè¯·æ±‚æ•°: {http.get('total_requests', 'N/A')}\n"
                prompt += f"- è®¿é—®åŸŸåæ•°: {http.get('unique_domains', 'N/A')}\n"
                prompt += f"- HTTPSæ¯”ä¾‹: {http.get('https_ratio', 0)*100:.1f}%\n"

        # æ ¹æ®ç­›é€‰æ¡ä»¶è¿‡æ»¤æŠ“åŒ…æ•°æ®
        filtered_capture_summary = self._filter_capture_data(capture_summary, filtered_domains, latency_filter)

        # æ·»åŠ ç­›é€‰ä¿¡æ¯
        if has_domain_filter or has_latency_filter:
            prompt += "\nğŸ” åˆ†æèŒƒå›´ç­›é€‰ï¼š\n"
            if has_domain_filter:
                prompt += f"- ç­›é€‰åŸŸå: {', '.join(filtered_domains)}\n"
            if has_latency_filter:
                filter_desc = {
                    'fast': 'å¿«é€Ÿç½‘ç«™ (å»¶è¿Ÿ â‰¤ 50ms)',
                    'slow': 'æ…¢é€Ÿç½‘ç«™ (å»¶è¿Ÿ > 100ms)',
                    'error': 'æœ‰é”™è¯¯çš„ç½‘ç«™ (é”™è¯¯ç‡ > 0%)'
                }
                prompt += f"- é€Ÿåº¦åˆ†ç±»: {filter_desc.get(latency_filter, latency_filter)}\n"

            prompt += "\nâš ï¸ é‡è¦æç¤ºï¼š\n"
            prompt += "1. ç”¨æˆ·å·²ç»ç­›é€‰äº†ç‰¹å®šçš„åŸŸåï¼Œè¯·åªåˆ†æä¸Šè¿°ç­›é€‰èŒƒå›´å†…çš„åŸŸå\n"
            prompt += "2. ä¸è¦åˆ†ææˆ–æåŠç­›é€‰èŒƒå›´ä¹‹å¤–çš„å…¶ä»–åŸŸå\n"
            prompt += "3. æ‰€æœ‰è¯Šæ–­ã€å»ºè®®å’ŒæŠ€æœ¯ç»†èŠ‚éƒ½åº”è¯¥ä¸“æ³¨äºç­›é€‰åçš„åŸŸå\n"
            prompt += "4. å¦‚æœç­›é€‰åçš„åŸŸåæ•°æ®ä¸è¶³ï¼Œè¯·æ˜ç¡®è¯´æ˜å¹¶åŸºäºç°æœ‰æ•°æ®è¿›è¡Œåˆ†æ\n\n"

        # æ·»åŠ è¯¦ç»†çš„æŠ“åŒ…æ•°æ®
        prompt += f"\nğŸ“‹ è¯¦ç»†æŠ“åŒ…æ•°æ®ï¼š\n{json.dumps(filtered_capture_summary, indent=2, ensure_ascii=False)}\n"

        # æ·»åŠ åˆ†æè¦æ±‚
        analysis_scope_note = ""
        if filtered_domains or (latency_filter and latency_filter != 'all'):
            analysis_scope_note = f"""
ğŸ¯ åˆ†æèŒƒå›´é™åˆ¶ï¼š
- æœ¬æ¬¡åˆ†æä»…é’ˆå¯¹ç”¨æˆ·ç­›é€‰çš„åŸŸåè¿›è¡Œ
- è¯·å‹¿åˆ†ææˆ–æåŠç­›é€‰èŒƒå›´å¤–çš„å…¶ä»–åŸŸå
- æ‰€æœ‰ç»“è®ºå’Œå»ºè®®éƒ½åº”åŸºäºç­›é€‰åçš„æ•°æ®

"""

        # æ ¹æ®æ˜¯å¦æœ‰ç­›é€‰æ¡ä»¶è°ƒæ•´åˆ†æè¦æ±‚
        if has_domain_filter or has_latency_filter:
            analysis_requirements = f"""{analysis_scope_note}ğŸ¯ åˆ†æè¦æ±‚ï¼š
1. é‡ç‚¹å…³æ³¨å¼‚å¸¸æŒ‡æ ‡å’Œæ€§èƒ½ç“¶é¢ˆï¼ˆä»…é™ç­›é€‰èŒƒå›´å†…çš„åŸŸåï¼‰
2. æä¾›å…·ä½“çš„æ•°å€¼åˆ†æå’Œå¯¹æ¯”ï¼ˆåŸºäºç­›é€‰åçš„æ•°æ®ï¼‰
3. ç»™å‡ºå¯æ“ä½œçš„è§£å†³æ­¥éª¤ï¼ˆé’ˆå¯¹ç­›é€‰çš„åŸŸåï¼‰
4. åŒ…å«é¢„é˜²æªæ–½å’Œç›‘æ§å»ºè®®
5. è¯„ä¼°é—®é¢˜çš„ç´§æ€¥ç¨‹åº¦å’Œå½±å“èŒƒå›´

è¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°JSONæ ¼å¼å›å¤ï¼Œç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½æœ‰å®é™…å†…å®¹ã€‚

âš ï¸ æœ€ç»ˆæé†’ï¼šç”¨æˆ·å·²ç­›é€‰ç‰¹å®šåŸŸåï¼Œè¯·ç¡®ä¿ï¼š
1. è¯Šæ–­ç»“è®ºåªåŸºäºç­›é€‰åçš„åŸŸåæ•°æ®
2. å…³é”®å‘ç°åªæ¶‰åŠç­›é€‰çš„åŸŸå
3. å»ºè®®æªæ–½åªé’ˆå¯¹ç­›é€‰çš„åŸŸå
4. æŠ€æœ¯ç»†èŠ‚åªåˆ†æç­›é€‰èŒƒå›´å†…çš„æ•°æ®
5. ä¸è¦æåŠæˆ–åˆ†æç­›é€‰èŒƒå›´å¤–çš„ä»»ä½•åŸŸå"""
        else:
            analysis_requirements = """ğŸ¯ åˆ†æè¦æ±‚ï¼š
1. é‡ç‚¹å…³æ³¨å¼‚å¸¸æŒ‡æ ‡å’Œæ€§èƒ½ç“¶é¢ˆ
2. æä¾›å…·ä½“çš„æ•°å€¼åˆ†æå’Œå¯¹æ¯”
3. ç»™å‡ºå¯æ“ä½œçš„è§£å†³æ­¥éª¤
4. åŒ…å«é¢„é˜²æªæ–½å’Œç›‘æ§å»ºè®®
5. è¯„ä¼°é—®é¢˜çš„ç´§æ€¥ç¨‹åº¦å’Œå½±å“èŒƒå›´

è¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°JSONæ ¼å¼å›å¤ï¼Œç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½æœ‰å®é™…å†…å®¹ï¼š"""

        prompt += analysis_requirements

        return prompt

    def _filter_capture_data(self, capture_summary: Dict, filtered_domains: Optional[List[str]] = None,
                           latency_filter: Optional[str] = None) -> Dict:
        """
        æ ¹æ®ç­›é€‰æ¡ä»¶è¿‡æ»¤æŠ“åŒ…æ•°æ®ï¼Œåªä¿ç•™ç”¨æˆ·å…³å¿ƒçš„åŸŸåæ•°æ®
        """
        if not filtered_domains and (not latency_filter or latency_filter == 'all'):
            # æ²¡æœ‰ç­›é€‰æ¡ä»¶ï¼Œè¿”å›åŸå§‹æ•°æ®
            return capture_summary

        # æ·±æ‹·è´åŸå§‹æ•°æ®
        filtered_data = copy.deepcopy(capture_summary)

        try:
            # è·å–ç½‘ç«™è®¿é—®æ•°æ®å’Œæ€§èƒ½æ•°æ®
            enhanced_analysis = filtered_data.get('enhanced_analysis', {})
            http_analysis = enhanced_analysis.get('http_analysis', {})
            websites_accessed = http_analysis.get('websites_accessed', {})

            issue_specific_insights = enhanced_analysis.get('issue_specific_insights', {})
            website_performance = issue_specific_insights.get('website_performance', {})

            # å¦‚æœæœ‰åŸŸåç­›é€‰ï¼Œå…ˆæŒ‰åŸŸåè¿‡æ»¤
            if filtered_domains:
                # è¿‡æ»¤ç½‘ç«™è®¿é—®æ•°æ®
                filtered_websites_accessed = {
                    domain: count for domain, count in websites_accessed.items()
                    if domain in filtered_domains
                }

                # è¿‡æ»¤ç½‘ç«™æ€§èƒ½æ•°æ®
                filtered_website_performance = {
                    domain: perf_data for domain, perf_data in website_performance.items()
                    if domain in filtered_domains
                }

                # æ›´æ–°æ•°æ®
                http_analysis['websites_accessed'] = filtered_websites_accessed
                issue_specific_insights['website_performance'] = filtered_website_performance

            # å¦‚æœæœ‰é€Ÿåº¦åˆ†ç±»ç­›é€‰ï¼Œè¿›ä¸€æ­¥è¿‡æ»¤
            if latency_filter and latency_filter != 'all':
                filtered_by_speed = {}

                for domain, perf_data in issue_specific_insights.get('website_performance', {}).items():
                    should_include = False

                    if latency_filter == 'fast':
                        # å¿«é€Ÿï¼šå»¶è¿Ÿ <= 50ms
                        latency = perf_data.get('tcp_rtt', {}).get('avg_ms', 0)
                        should_include = latency > 0 and latency <= 50
                    elif latency_filter == 'slow':
                        # æ…¢é€Ÿï¼šå»¶è¿Ÿ > 100ms
                        latency = perf_data.get('tcp_rtt', {}).get('avg_ms', 0)
                        should_include = latency > 100
                    elif latency_filter == 'error':
                        # é”™è¯¯ï¼šé”™è¯¯ç‡ > 0%
                        error_rate = perf_data.get('requests', {}).get('error_rate_percent', 0)
                        should_include = error_rate > 0

                    if should_include:
                        filtered_by_speed[domain] = perf_data

                # æ›´æ–°æ€§èƒ½æ•°æ®
                issue_specific_insights['website_performance'] = filtered_by_speed

                # åŒæ­¥æ›´æ–°è®¿é—®æ•°æ®
                filtered_websites_accessed = {
                    domain: count for domain, count in http_analysis.get('websites_accessed', {}).items()
                    if domain in filtered_by_speed
                }
                http_analysis['websites_accessed'] = filtered_websites_accessed

            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            remaining_domains = len(http_analysis.get('websites_accessed', {}))
            if 'connection_summary' in http_analysis:
                http_analysis['connection_summary']['total_websites'] = remaining_domains
                http_analysis['connection_summary']['filtered_analysis'] = True

            logger.info(f"åŸŸåç­›é€‰å®Œæˆï¼Œå‰©ä½™åŸŸåæ•°é‡: {remaining_domains}")

        except Exception as e:
            logger.warning(f"æ•°æ®ç­›é€‰å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ•°æ®: {str(e)}")
            return capture_summary

        return filtered_data

    def _call_ai_api_sync(self, prompt: str) -> str:
        """åŒæ­¥ç‰ˆæœ¬çš„AI APIè°ƒç”¨"""
        try:
            config = self.ai_config.get_current_config()
            logger.info(f"è·å–AIé…ç½®: {config is not None}")

            if not config:
                logger.error("AIé…ç½®æ— æ•ˆæˆ–æœªæ‰¾åˆ°")
                raise Exception("AIé…ç½®æ— æ•ˆï¼šè¯·æ£€æŸ¥ç¯å¢ƒå˜é‡è®¾ç½®")

            logger.info(f"è°ƒç”¨AI APIï¼Œæä¾›å•†: {self.ai_config.current_provider}")
            logger.info(f"APIé…ç½® - æ¨¡å‹: {config.model}, åŸºç¡€URL: {config.base_url}")

            if self.ai_config.current_provider == 'openrouter':
                result = self._call_openrouter_api_sync(prompt, config)
            elif self.ai_config.current_provider == 'openai':
                result = self._call_openai_api_sync(prompt, config)
            elif self.ai_config.current_provider == 'anthropic':
                result = self._call_anthropic_api_sync(prompt, config)
            else:
                raise Exception(f"ä¸æ”¯æŒçš„AIæä¾›å•†: {self.ai_config.current_provider}")

            logger.info("AI APIè°ƒç”¨æˆåŠŸ")
            return result

        except Exception as e:
            logger.error(f"AI APIè°ƒç”¨å¤±è´¥: {str(e)}", exc_info=True)
            raise

    async def _call_ai_api(self, prompt: str) -> str:
        """è°ƒç”¨AI API"""
        config = self.ai_config.get_current_config()
        if not config:
            raise Exception("AIé…ç½®æ— æ•ˆ")
        
        try:
            if self.ai_config.current_provider == 'openrouter':
                return await self._call_openrouter_api(prompt, config)
            elif self.ai_config.current_provider == 'openai':
                return await self._call_openai_api(prompt, config)
            elif self.ai_config.current_provider == 'anthropic':
                return await self._call_anthropic_api(prompt, config)
            else:
                raise Exception(f"ä¸æ”¯æŒçš„AIæä¾›å•†: {self.ai_config.current_provider}")
                
        except Exception as e:
            logger.error(f"AI APIè°ƒç”¨å¤±è´¥: {str(e)}")
            raise
    
    async def _call_openrouter_api(self, prompt: str, config) -> str:
        """è°ƒç”¨OpenRouter API"""
        import aiohttp

        headers = {
            'Authorization': f'Bearer {config.api_key}',
            'Content-Type': 'application/json',
            'HTTP-Referer': 'http://localhost:3000',
            'X-Title': 'Network Packet Analysis'
        }

        data = {
            'model': config.model,
            'messages': [
                {'role': 'user', 'content': prompt}
            ],
            'max_tokens': config.max_tokens,
            'temperature': config.temperature
        }

        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{config.base_url}/chat/completions",
                headers=headers,
                json=data,
                timeout=aiohttp.ClientTimeout(total=config.timeout)
            ) as response:
                if response.status != 200:
                    error_text = await response.text()
                    raise Exception(f"OpenRouter APIé”™è¯¯: {response.status} - {error_text}")

                result = await response.json()
                return result['choices'][0]['message']['content']

    def _call_openrouter_api_sync(self, prompt: str, config) -> str:
        """åŒæ­¥è°ƒç”¨OpenRouter API"""
        import requests

        try:
            logger.info(f"å‡†å¤‡è°ƒç”¨OpenRouter API: {config.base_url}")
            logger.info(f"ä½¿ç”¨æ¨¡å‹: {config.model}")
            logger.info(f"Prompté•¿åº¦: {len(prompt)} å­—ç¬¦")
            headers = {
                'Authorization': f'Bearer {config.api_key}',  # éšè—APIå¯†é’¥
                'Content-Type': 'application/json',
                'HTTP-Referer': 'http://localhost:3000',
                'X-Title': 'Network Packet Analysis'
            }

            data = {
                'model': config.model,
                'messages': [
                    {'role': 'user', 'content': prompt}
                ],
                'max_tokens': config.max_tokens,
                'temperature': config.temperature
            }

            logger.info(f"å‘é€è¯·æ±‚åˆ°: {config.base_url}/chat/completions")

            response = requests.post(
                f"{config.base_url}/chat/completions",
                headers=headers,
                json=data,
                timeout=config.timeout
            )

            logger.info(f"APIå“åº”çŠ¶æ€ç : {response.status_code}")

            if response.status_code != 200:
                error_text = response.text
                logger.error(f"APIé”™è¯¯å“åº”: {error_text}")
                raise Exception(f"OpenRouter APIé”™è¯¯: {response.status_code} - {error_text}")

            result = response.json()
            logger.info("APIå“åº”è§£ææˆåŠŸ")

            if 'choices' not in result or not result['choices']:
                logger.error(f"APIå“åº”æ ¼å¼å¼‚å¸¸: {result}")
                raise Exception("APIå“åº”ä¸­æ²¡æœ‰choiceså­—æ®µ")

            content = result['choices'][0]['message']['content']
            logger.info(f"è·å–åˆ°AIå“åº”ï¼Œé•¿åº¦: {len(content)} å­—ç¬¦")

            return content

        except requests.exceptions.RequestException as e:
            logger.error(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}")
            raise Exception(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}")
        except Exception as e:
            logger.error(f"OpenRouter APIè°ƒç”¨å¤±è´¥: {str(e)}")
            raise

    def _call_openai_api_sync(self, prompt: str, config) -> str:
        """åŒæ­¥è°ƒç”¨OpenAI API"""
        import openai

        client = openai.OpenAI(
            api_key=config.api_key,
            base_url=config.base_url
        )

        response = client.chat.completions.create(
            model=config.model,
            messages=[
                {'role': 'user', 'content': prompt}
            ],
            max_tokens=config.max_tokens,
            temperature=config.temperature
        )

        return response.choices[0].message.content

    def _call_anthropic_api_sync(self, prompt: str, config) -> str:
        """åŒæ­¥è°ƒç”¨Anthropic API"""
        import anthropic

        client = anthropic.Anthropic(api_key=config.api_key)

        response = client.messages.create(
            model=config.model,
            max_tokens=config.max_tokens,
            temperature=config.temperature,
            messages=[
                {'role': 'user', 'content': prompt}
            ]
        )

        return response.content[0].text

    async def _call_openai_api(self, prompt: str, config) -> str:
        """è°ƒç”¨OpenAI API"""
        client = openai.OpenAI(
            api_key=config.api_key,
            base_url=config.base_url
        )
        
        response = client.chat.completions.create(
            model=config.model,
            messages=[
                {'role': 'user', 'content': prompt}
            ],
            max_tokens=config.max_tokens,
            temperature=config.temperature
        )
        
        return response.choices[0].message.content
    
    async def _call_anthropic_api(self, prompt: str, config) -> str:
        """è°ƒç”¨Anthropic API"""
        client = anthropic.Anthropic(api_key=config.api_key)
        
        response = client.messages.create(
            model=config.model,
            max_tokens=config.max_tokens,
            temperature=config.temperature,
            messages=[
                {'role': 'user', 'content': prompt}
            ]
        )

        return response.content[0].text

    def _validate_analysis_result(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯å’Œè¡¥å……åˆ†æç»“æœ"""
        # éªŒè¯å’Œè¡¥å……å¿…è¦å­—æ®µ
        required_fields = {
            'diagnosis': 'ç½‘ç»œé—®é¢˜åˆ†æä¸­',
            'severity': 'medium',
            'root_cause': 'æ­£åœ¨åˆ†ææ ¹æœ¬åŸå› ',
            'recommendations': ['è¯·ç¨åæŸ¥çœ‹è¯¦ç»†åˆ†æç»“æœ'],
            'technical_details': 'æŠ€æœ¯åˆ†æè¿›è¡Œä¸­',
            'confidence': 70
        }

        for field, default_value in required_fields.items():
            if field not in analysis:
                analysis[field] = default_value

        # ç¡®ä¿åˆ—è¡¨å­—æ®µæ˜¯åˆ—è¡¨æ ¼å¼
        list_fields = ['recommendations', 'key_findings', 'diagnostic_clues']
        for field in list_fields:
            if field in analysis and not isinstance(analysis[field], list):
                analysis[field] = [str(analysis[field])] if analysis[field] else []
            elif field not in analysis:
                analysis[field] = []

        # ç¡®ä¿æ•°å€¼å­—æ®µæ˜¯æ•°å­—
        if 'confidence' in analysis:
            try:
                analysis['confidence'] = int(analysis['confidence'])
            except (ValueError, TypeError):
                analysis['confidence'] = 70

        return analysis

    def _parse_ai_response(self, ai_response: str, issue_type: str) -> Dict[str, Any]:
        """è§£æAIå“åº”"""
        try:
            logger.info(f"å¼€å§‹è§£æAIå“åº”ï¼Œé•¿åº¦: {len(ai_response)} å­—ç¬¦")
            logger.debug(f"AIå“åº”å‰100å­—ç¬¦: {ai_response[:100]}")

            # æ¸…ç†å“åº”æ–‡æœ¬ - ç§»é™¤æ§åˆ¶å­—ç¬¦å’Œä¸å¯è§å­—ç¬¦
            import re
            # ç§»é™¤æ§åˆ¶å­—ç¬¦ï¼ˆé™¤äº†æ¢è¡Œç¬¦ã€åˆ¶è¡¨ç¬¦å’Œå›è½¦ç¬¦ï¼‰
            cleaned_response = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', ai_response)
            # æ¸…ç†é¦–å°¾ç©ºç™½
            cleaned_response = cleaned_response.strip()

            logger.debug(f"æ¸…ç†åå“åº”å‰100å­—ç¬¦: {cleaned_response[:100]}")

            # å°è¯•è§£æJSONå“åº”
            if cleaned_response.startswith('{') and cleaned_response.endswith('}'):
                logger.info("æ£€æµ‹åˆ°å®Œæ•´JSONæ ¼å¼å“åº”")
                analysis = json.loads(cleaned_response)
            else:
                # å¦‚æœä¸æ˜¯å®Œæ•´JSONæ ¼å¼ï¼Œå°è¯•æå–JSONéƒ¨åˆ†
                import re
                logger.info("å°è¯•ä»å“åº”ä¸­æå–JSONéƒ¨åˆ†")

                # æ›´å¼ºçš„JSONæå–æ¨¡å¼
                json_patterns = [
                    r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}',  # ç®€å•åµŒå¥—JSON
                    r'\{.*?\}(?=\s*$)',  # åˆ°ç»“å°¾çš„JSON
                    r'\{.*\}',  # æœ€å®½æ³›çš„åŒ¹é…
                ]

                analysis = None
                for pattern in json_patterns:
                    json_match = re.search(pattern, cleaned_response, re.DOTALL)
                    if json_match:
                        try:
                            json_text = json_match.group()
                            logger.info(f"æ‰¾åˆ°JSONåŒ¹é…ï¼Œé•¿åº¦: {len(json_text)} å­—ç¬¦")
                            analysis = json.loads(json_text)
                            logger.info("JSONè§£ææˆåŠŸ")
                            break
                        except json.JSONDecodeError as je:
                            logger.warning(f"JSONè§£æå¤±è´¥: {str(je)}")
                            continue

                if analysis is None:
                    # å¦‚æœæ— æ³•è§£æJSONï¼Œåˆ›å»ºé»˜è®¤ç»“æ„å¹¶åŒ…å«AIçš„æ–‡æœ¬å“åº”
                    logger.info("æ— æ³•è§£æJSONï¼Œåˆ›å»ºé»˜è®¤ç»“æ„")
                    analysis = {
                        'diagnosis': ai_response[:200] + '...' if len(ai_response) > 200 else ai_response,
                        'severity': 'medium',
                        'root_cause': 'éœ€è¦è¿›ä¸€æ­¥åˆ†æï¼ŒAIè¿”å›äº†éç»“æ„åŒ–å“åº”',
                        'key_findings': ['AIå·²æä¾›åˆ†æï¼Œä½†æ ¼å¼éœ€è¦è°ƒæ•´'],
                        'recommendations': [
                            'è¯·æŸ¥çœ‹æŠ€æœ¯ç»†èŠ‚ä¸­çš„å®Œæ•´AIåˆ†æ',
                            'å»ºè®®é‡æ–°è¿›è¡Œç»“æ„åŒ–åˆ†æ',
                            'å¦‚éœ€è¦ï¼Œè¯·è”ç³»ç½‘ç»œç®¡ç†å‘˜'
                        ],
                        'diagnostic_clues': [
                            'ğŸ“‹ AIå·²å®Œæˆåˆ†æä½†è¿”å›æ ¼å¼ä¸æ ‡å‡†',
                            'ğŸ” è¯·æŸ¥çœ‹æŠ€æœ¯ç»†èŠ‚è·å–å®Œæ•´ä¿¡æ¯',
                            'âš ï¸ å»ºè®®ä½¿ç”¨ç»“æ„åŒ–åˆ†ææ¨¡å¼'
                        ],
                        'technical_details': ai_response,
                        'confidence': 60,
                        'next_steps': 'å»ºè®®é‡æ–°è¿›è¡Œåˆ†æä»¥è·å¾—ç»“æ„åŒ–ç»“æœ'
                    }
            
            return self._validate_analysis_result(analysis)

        except json.JSONDecodeError as e:
            logger.error(f"JSONè§£æå¤±è´¥: {str(e)}", exc_info=True)

            # è¯¦ç»†çš„JSONé”™è¯¯è°ƒè¯•
            try:
                # æ‰¾åˆ°é”™è¯¯ä½ç½®çš„å­—ç¬¦
                error_char = ai_response[e.pos] if e.pos < len(ai_response) else 'EOF'
                error_context = ai_response[max(0, e.pos-20):e.pos+20] if e.pos < len(ai_response) else ai_response[-40:]

                logger.error(f"JSONé”™è¯¯ä½ç½®: ç¬¬{e.lineno}è¡Œç¬¬{e.colno}åˆ—")
                logger.error(f"é”™è¯¯å­—ç¬¦: '{error_char}' (ASCII: {ord(error_char) if error_char != 'EOF' else 'EOF'})")
                logger.error(f"é”™è¯¯ä¸Šä¸‹æ–‡: {repr(error_context)}")

                # å°è¯•æ›´æ¿€è¿›çš„æ¸…ç†
                import re
                super_cleaned = re.sub(r'[\x00-\x1F\x7F-\x9F]', '', ai_response)  # ç§»é™¤æ‰€æœ‰æ§åˆ¶å­—ç¬¦
                super_cleaned = re.sub(r'[^\x20-\x7E\u4e00-\u9fff]', '', super_cleaned)  # åªä¿ç•™å¯æ‰“å°ASCIIå’Œä¸­æ–‡
                super_cleaned = super_cleaned.strip()

                logger.info("å°è¯•è¶…çº§æ¸…ç†åé‡æ–°è§£æJSON")
                if super_cleaned.startswith('{') and super_cleaned.endswith('}'):
                    analysis = json.loads(super_cleaned)
                    logger.info("è¶…çº§æ¸…ç†åJSONè§£ææˆåŠŸ")
                    return self._validate_analysis_result(analysis)

            except Exception as cleanup_error:
                logger.error(f"æ¸…ç†é‡è¯•ä¹Ÿå¤±è´¥: {cleanup_error}")

            # JSONè§£æå¤±è´¥çš„è°ƒè¯•ä¿¡æ¯
            debug_info = {
                'error': str(e),
                'error_type': 'JSONDecodeError',
                'line': e.lineno,
                'column': e.colno,
                'position': e.pos,
                'ai_response_length': len(ai_response),
                'ai_response_preview': ai_response[:300],
                'issue_type': issue_type,
                'timestamp': datetime.now().isoformat()
            }
            logger.error(f"JSONè§£æè°ƒè¯•ä¿¡æ¯: {debug_info}")

        except Exception as e:
            logger.error(f"è§£æAIå“åº”å¤±è´¥: {str(e)}", exc_info=True)
            logger.error(f"AIå“åº”å†…å®¹: {ai_response[:500]}...")

            # å°è¯•ä¿å­˜è°ƒè¯•ä¿¡æ¯
            try:
                debug_info = {
                    'error': str(e),
                    'error_type': type(e).__name__,
                    'ai_response_length': len(ai_response),
                    'ai_response_preview': ai_response[:200],
                    'issue_type': issue_type,
                    'timestamp': datetime.now().isoformat()
                }
                logger.error(f"è°ƒè¯•ä¿¡æ¯: {debug_info}")
            except:
                pass

            return {
                'diagnosis': 'åˆ†æç»“æœè§£æå¤±è´¥ï¼Œä½†å·²è·å–åˆ°AIå“åº”',
                'severity': 'medium',
                'root_cause': f'å“åº”è§£æé”™è¯¯: {str(e)}ï¼ŒåŸå§‹å“åº”é•¿åº¦: {len(ai_response)}å­—ç¬¦',
                'key_findings': ['AIåˆ†æå·²å®Œæˆï¼Œä½†å“åº”æ ¼å¼éœ€è¦è°ƒæ•´'],
                'recommendations': [
                    'è¯·æ£€æŸ¥AIæœåŠ¡é…ç½®',
                    'å°è¯•é‡æ–°è¿›è¡Œåˆ†æ',
                    'å¦‚é—®é¢˜æŒç»­ï¼Œè¯·è”ç³»æŠ€æœ¯æ”¯æŒ'
                ],
                'diagnostic_clues': [
                    f'ğŸ“‹ AIå“åº”é•¿åº¦: {len(ai_response)}å­—ç¬¦',
                    f'ğŸ” è§£æé”™è¯¯: {str(e)}',
                    f'ğŸ”§ é”™è¯¯ç±»å‹: {type(e).__name__}',
                    'âš ï¸ å»ºè®®æ£€æŸ¥AIæœåŠ¡çŠ¶æ€'
                ],
                'technical_details': f'è§£æé”™è¯¯è¯¦æƒ…: {str(e)}\né”™è¯¯ç±»å‹: {type(e).__name__}\n\nåŸå§‹AIå“åº”:\n{ai_response[:1000]}...' if len(ai_response) > 1000 else f'è§£æé”™è¯¯è¯¦æƒ…: {str(e)}\né”™è¯¯ç±»å‹: {type(e).__name__}\n\nåŸå§‹AIå“åº”:\n{ai_response}',
                'confidence': 30,
                'next_steps': 'å»ºè®®é‡æ–°è¿›è¡Œåˆ†ææˆ–è”ç³»æŠ€æœ¯æ”¯æŒ'
            }

# å…¨å±€AIåˆ†ææœåŠ¡å®ä¾‹
ai_analysis_service = AIAnalysisService()

def get_ai_analysis_service() -> AIAnalysisService:
    """è·å–AIåˆ†ææœåŠ¡å®ä¾‹"""
    return ai_analysis_service
