import os
import uuid
import threading
import time
import subprocess
import logging
import asyncio
from fastapi import APIRouter, BackgroundTasks, Query, HTTPException
from pydantic import BaseModel
from typing import Dict, Optional, List
import pyshark
import json
from datetime import datetime

from app.services.ai_analysis_service import get_ai_analysis_service

logger = logging.getLogger(__name__)

router = APIRouter()

CAPTURE_DIR = '/tmp/packet_captures'
os.makedirs(CAPTURE_DIR, exist_ok=True)
tasks: Dict[str, Dict] = {}

class CaptureRequest(BaseModel):
    issue_type: str
    duration: int = 10
    interface: Optional[str] = None  # å°†åœ¨å¤„ç†æ—¶è‡ªåŠ¨è®¾ç½®
    target_ip: Optional[str] = None
    target_port: Optional[int] = None
    custom_filter: Optional[str] = None
    user_description: Optional[str] = None
    enable_ai_analysis: bool = True

@router.post('')
def start_capture(req: CaptureRequest, background_tasks: BackgroundTasks):
    """å¯åŠ¨ç½‘ç»œæŠ“åŒ…ä»»åŠ¡"""
    try:
        # å¦‚æœæ²¡æœ‰æŒ‡å®šæ¥å£ï¼Œä½¿ç”¨é»˜è®¤æ¥å£
        if not req.interface:
            req.interface = get_default_interface()
            logger.info(f"ä½¿ç”¨é»˜è®¤ç½‘ç»œæ¥å£: {req.interface}")

        # éªŒè¯æ¥å£æ˜¯å¦å­˜åœ¨
        if not validate_interface(req.interface):
            # å¦‚æœæŒ‡å®šçš„æ¥å£ä¸å­˜åœ¨ï¼Œå°è¯•ä½¿ç”¨é»˜è®¤æ¥å£
            default_interface = get_default_interface()
            if validate_interface(default_interface):
                logger.warning(f"æ¥å£ {req.interface} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤æ¥å£: {default_interface}")
                req.interface = default_interface
            else:
                available_interfaces = list_available_interfaces()
                error_msg = f"ç½‘ç»œæ¥å£ {req.interface} ä¸å­˜åœ¨"
                if available_interfaces:
                    error_msg += f"ï¼Œå¯ç”¨æ¥å£: {', '.join(available_interfaces[:5])}"
                raise HTTPException(status_code=400, detail=error_msg)

        # éªŒè¯æŠ“åŒ…æ—¶é•¿
        if req.duration < 1 or req.duration > 300:
            raise HTTPException(status_code=400, detail="æŠ“åŒ…æ—¶é•¿å¿…é¡»åœ¨1-300ç§’ä¹‹é—´")

        task_id = str(uuid.uuid4())
        tasks[task_id] = {
            'status': 'pending',
            'result': None,
            'error': None,
            'created_at': datetime.now().isoformat(),
            'request': req.dict()
        }

        background_tasks.add_task(run_capture_wrapper, task_id, req)
        logger.info(f"åˆ›å»ºæŠ“åŒ…ä»»åŠ¡: {task_id}, é—®é¢˜ç±»å‹: {req.issue_type}")

        return {'task_id': task_id, 'status': 'pending'}

    except Exception as e:
        logger.error(f"å¯åŠ¨æŠ“åŒ…ä»»åŠ¡å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"å¯åŠ¨æŠ“åŒ…ä»»åŠ¡å¤±è´¥: {str(e)}")

@router.get('/status')
def get_status(task_id: str = Query(...)):
    """è·å–æŠ“åŒ…ä»»åŠ¡çŠ¶æ€"""
    task = tasks.get(task_id)
    if not task:
        return {'status': 'not_found', 'error': 'ä»»åŠ¡ä¸å­˜åœ¨'}

    return {
        'status': task['status'],
        'error': task.get('error'),
        'created_at': task.get('created_at'),
        'progress': get_task_progress(task['status'])
    }

@router.get('/result')
def get_result(task_id: str = Query(...)):
    """è·å–æŠ“åŒ…åˆ†æç»“æœ"""
    task = tasks.get(task_id)
    if not task:
        return {'error': 'ä»»åŠ¡ä¸å­˜åœ¨'}

    if task['status'] != 'done':
        return {'error': 'ä»»åŠ¡æœªå®Œæˆ', 'status': task['status']}

    if not task.get('result'):
        return {'error': 'ç»“æœä¸å¯ç”¨'}

    return {
        'result': task['result'],
        'status': 'done',
        'created_at': task.get('created_at')
    }

@router.get('/interfaces')
def get_network_interfaces():
    """è·å–å¯ç”¨çš„ç½‘ç»œæ¥å£"""
    try:
        import platform
        interfaces = list_available_interfaces()
        default_interface = get_default_interface()

        return {
            'interfaces': interfaces,
            'default': default_interface,
            'current_system': platform.system().lower()
        }
    except Exception as e:
        logger.error(f"è·å–ç½‘ç»œæ¥å£å¤±è´¥: {str(e)}")
        import platform
        return {
            'interfaces': [],
            'default': get_default_interface(),
            'current_system': platform.system().lower(),
            'error': str(e)
        }

def run_capture_wrapper(task_id: str, req: CaptureRequest):
    """åŒæ­¥åŒ…è£…å‡½æ•°ï¼Œç”¨äºåœ¨åå°ä»»åŠ¡ä¸­è¿è¡Œå¼‚æ­¥æŠ“åŒ…å‡½æ•°"""
    try:
        # åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

        # è¿è¡Œå¼‚æ­¥å‡½æ•°
        loop.run_until_complete(run_capture(task_id, req))

    except Exception as e:
        logger.error(f"æŠ“åŒ…åŒ…è£…å‡½æ•°æ‰§è¡Œå¤±è´¥: {task_id}, é”™è¯¯: {str(e)}")
        tasks[task_id]['status'] = 'error'
        tasks[task_id]['error'] = str(e)
    finally:
        # æ¸…ç†äº‹ä»¶å¾ªç¯
        try:
            loop.close()
        except:
            pass

async def run_capture(task_id: str, req: CaptureRequest):
    """æ‰§è¡ŒæŠ“åŒ…ä»»åŠ¡"""
    try:
        logger.info(f"å¼€å§‹æ‰§è¡ŒæŠ“åŒ…ä»»åŠ¡: {task_id}")

        # 1. ç”ŸæˆæŠ“åŒ…å‘½ä»¤å’Œæ–‡ä»¶è·¯å¾„
        pcap_file = os.path.join(CAPTURE_DIR, f'{task_id}.pcap')
        filter_expr = get_filter_by_issue(req.issue_type, req.target_ip, req.target_port, req.custom_filter)

        # æ„å»ºtcpdumpå‘½ä»¤
        cmd = build_tcpdump_command(req.interface, pcap_file, req.duration, filter_expr)

        # 2. æ‰§è¡ŒæŠ“åŒ…
        tasks[task_id]['status'] = 'capturing'
        logger.info(f"æ‰§è¡ŒæŠ“åŒ…å‘½ä»¤: {cmd}")

        # åœ¨macOSä¸‹ï¼Œæˆ‘ä»¬éœ€è¦æ‰‹åŠ¨ç»ˆæ­¢tcpdumpè¿›ç¨‹
        import platform
        system = platform.system().lower()

        if system == 'darwin':
            # macOSä¸‹å¯åŠ¨tcpdumpè¿›ç¨‹å¹¶åœ¨æŒ‡å®šæ—¶é—´åç»ˆæ­¢
            import signal
            import time

            process = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)

            # ç­‰å¾…æŒ‡å®šçš„æŠ“åŒ…æ—¶é—´
            time.sleep(req.duration)

            # ç»ˆæ­¢è¿›ç¨‹
            try:
                process.terminate()
                stdout, stderr = process.communicate(timeout=5)
                result = subprocess.CompletedProcess(cmd, process.returncode, stdout, stderr)
            except subprocess.TimeoutExpired:
                process.kill()
                stdout, stderr = process.communicate()
                result = subprocess.CompletedProcess(cmd, process.returncode, stdout, stderr)
        else:
            # Linuxä¸‹ä½¿ç”¨åŸæ¥çš„æ–¹å¼
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=req.duration + 30)

        if result.returncode != 0:
            raise Exception(f"æŠ“åŒ…å‘½ä»¤æ‰§è¡Œå¤±è´¥: {result.stderr}")

        # æ£€æŸ¥pcapæ–‡ä»¶æ˜¯å¦ç”Ÿæˆ
        if not os.path.exists(pcap_file) or os.path.getsize(pcap_file) == 0:
            raise Exception("æŠ“åŒ…æ–‡ä»¶æœªç”Ÿæˆæˆ–ä¸ºç©º")

        # 3. é¢„å¤„ç†æ•°æ®åŒ…
        tasks[task_id]['status'] = 'processing'
        logger.info(f"å¼€å§‹é¢„å¤„ç†æ•°æ®åŒ…: {pcap_file}")

        summary = preprocess_pcap(pcap_file, req.issue_type)

        # 4. AIåˆ†æï¼ˆå¦‚æœå¯ç”¨ï¼‰
        ai_analysis = None
        if req.enable_ai_analysis:
            try:
                tasks[task_id]['status'] = 'ai_analyzing'
                logger.info(f"å¼€å§‹AIåˆ†æ: {task_id}")

                # åœ¨æ–°çš„çº¿ç¨‹ä¸­æ‰§è¡ŒAIåˆ†æï¼Œé¿å…äº‹ä»¶å¾ªç¯å†²çª
                import concurrent.futures
                import threading

                def run_ai_analysis():
                    try:
                        logger.info("AIåˆ†æçº¿ç¨‹å¼€å§‹æ‰§è¡Œ")
                        ai_service = get_ai_analysis_service()

                        # ä½¿ç”¨åŒæ­¥æ–¹å¼è°ƒç”¨AIåˆ†æï¼Œä¼ é€’task_idç”¨äºè°ƒè¯•
                        result = ai_service.analyze_network_issue_sync(
                            req.issue_type,
                            summary,
                            req.user_description,
                            task_id
                        )
                        logger.info("AIåˆ†æçº¿ç¨‹æ‰§è¡Œå®Œæˆ")
                        return result

                    except Exception as e:
                        logger.error(f"AIåˆ†æçº¿ç¨‹å¼‚å¸¸: {str(e)}", exc_info=True)
                        return {
                            'success': False,
                            'error': f"AIåˆ†æçº¿ç¨‹å¼‚å¸¸: {str(e)}",
                            'timestamp': datetime.now().isoformat()
                        }

                # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒAIåˆ†æ
                try:
                    with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                        logger.info("æäº¤AIåˆ†æä»»åŠ¡åˆ°çº¿ç¨‹æ± ")
                        future = executor.submit(run_ai_analysis)
                        ai_analysis = future.result(timeout=30)  # 30ç§’è¶…æ—¶
                        logger.info(f"AIåˆ†æå®Œæˆ: {task_id}")

                except concurrent.futures.TimeoutError:
                    logger.error(f"AIåˆ†æè¶…æ—¶: {task_id}")
                    ai_analysis = {
                        'success': False,
                        'error': 'AIåˆ†æè¶…æ—¶ï¼ˆ30ç§’ï¼‰',
                        'timestamp': datetime.now().isoformat()
                    }
                except Exception as e:
                    logger.error(f"çº¿ç¨‹æ± æ‰§è¡Œå¤±è´¥: {task_id}, é”™è¯¯: {str(e)}", exc_info=True)
                    ai_analysis = {
                        'success': False,
                        'error': f"çº¿ç¨‹æ± æ‰§è¡Œå¤±è´¥: {str(e)}",
                        'timestamp': datetime.now().isoformat()
                    }

            except Exception as e:
                logger.error(f"AIåˆ†æå¤–å±‚å¼‚å¸¸: {task_id}, é”™è¯¯: {str(e)}", exc_info=True)
                ai_analysis = {
                    'success': False,
                    'error': f"AIåˆ†æå¤–å±‚å¼‚å¸¸: {str(e)}",
                    'timestamp': datetime.now().isoformat()
                }

        # 5. å®Œæˆä»»åŠ¡ - ç¡®ä¿çŠ¶æ€å§‹ç»ˆè¢«æ›´æ–°
        try:
            result = {
                'capture_summary': summary,
                'ai_analysis': ai_analysis,
                'task_info': {
                    'task_id': task_id,
                    'issue_type': req.issue_type,
                    'duration': req.duration,
                    'interface': req.interface,
                    'completed_at': datetime.now().isoformat()
                }
            }

            tasks[task_id]['result'] = result
            tasks[task_id]['status'] = 'done'

            logger.info(f"æŠ“åŒ…ä»»åŠ¡å®Œæˆ: {task_id}")

        except Exception as e:
            logger.error(f"è®¾ç½®ä»»åŠ¡å®ŒæˆçŠ¶æ€å¤±è´¥: {task_id}, é”™è¯¯: {str(e)}", exc_info=True)
            # å³ä½¿è®¾ç½®ç»“æœå¤±è´¥ï¼Œä¹Ÿè¦ç¡®ä¿çŠ¶æ€è¢«æ›´æ–°
            tasks[task_id]['status'] = 'error'
            tasks[task_id]['error'] = f"è®¾ç½®ä»»åŠ¡ç»“æœå¤±è´¥: {str(e)}"

    except subprocess.TimeoutExpired:
        tasks[task_id]['status'] = 'error'
        tasks[task_id]['error'] = 'æŠ“åŒ…è¶…æ—¶'
        logger.error(f"æŠ“åŒ…ä»»åŠ¡è¶…æ—¶: {task_id}")

    except Exception as e:
        tasks[task_id]['status'] = 'error'
        tasks[task_id]['error'] = str(e)
        logger.error(f"æŠ“åŒ…ä»»åŠ¡å¤±è´¥: {task_id}, é”™è¯¯: {str(e)}")

    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼ˆå¯é€‰ï¼Œä¿ç•™ç”¨äºè°ƒè¯•ï¼‰
        # if os.path.exists(pcap_file):
        #     os.remove(pcap_file)
        pass

def get_filter_by_issue(issue_type: str, target_ip: Optional[str] = None,
                       target_port: Optional[int] = None, custom_filter: Optional[str] = None) -> str:
    """æ ¹æ®é—®é¢˜ç±»å‹ç”ŸæˆæŠ“åŒ…è¿‡æ»¤è¡¨è¾¾å¼"""

    # å¦‚æœæœ‰è‡ªå®šä¹‰è¿‡æ»¤å™¨ï¼Œä¼˜å…ˆä½¿ç”¨
    if custom_filter:
        return custom_filter

    # é‡æ–°è®¾è®¡çš„è¿‡æ»¤è¡¨è¾¾å¼ - ä¸“æ³¨äºä¸‰å¤§æ ¸å¿ƒåŠŸèƒ½
    base_filters = {
        'website_access': 'tcp port 80 or port 443 or port 8080 or port 8443',  # ç½‘ç«™è®¿é—®é—®é¢˜
        'interconnection': 'tcp or udp',  # äº’è”äº’é€šè®¿é—®ï¼ˆéœ€è¦å…¨æµé‡åˆ†æï¼‰
        'game_lag': 'udp or (tcp and (port 7000 or port 8001 or port 17500 or port 27015 or port 25565 or port 10012))',  # æ¸¸æˆå¡é¡¿é—®é¢˜
        'custom': ''
    }

    filter_expr = base_filters.get(issue_type, '')

    # æ·»åŠ ç›®æ ‡IPè¿‡æ»¤
    if target_ip:
        ip_filter = f"host {target_ip}"
        if filter_expr:
            filter_expr = f"({filter_expr}) and {ip_filter}"
        else:
            filter_expr = ip_filter

    # æ·»åŠ ç›®æ ‡ç«¯å£è¿‡æ»¤
    if target_port:
        port_filter = f"port {target_port}"
        if filter_expr:
            filter_expr = f"({filter_expr}) and {port_filter}"
        else:
            filter_expr = port_filter

    return filter_expr

def validate_interface(interface: str) -> bool:
    """éªŒè¯ç½‘ç»œæ¥å£æ˜¯å¦å­˜åœ¨"""
    try:
        import platform
        system = platform.system().lower()

        if system == 'darwin':  # macOS
            result = subprocess.run(['ifconfig', interface],
                                  capture_output=True, text=True)
            return result.returncode == 0
        else:  # Linux
            result = subprocess.run(['ip', 'link', 'show', interface],
                                  capture_output=True, text=True)
            return result.returncode == 0
    except Exception:
        return False

def get_default_interface() -> str:
    """è·å–é»˜è®¤ç½‘ç»œæ¥å£"""
    try:
        import platform
        system = platform.system().lower()

        if system == 'darwin':  # macOS
            # å°è¯•å¸¸è§çš„macOSæ¥å£
            common_interfaces = ['en0', 'en1', 'en2', 'lo0']
            for interface in common_interfaces:
                if validate_interface(interface):
                    return interface
            return 'en0'  # é»˜è®¤è¿”å›en0
        else:  # Linux
            # å°è¯•å¸¸è§çš„Linuxæ¥å£
            common_interfaces = ['eth0', 'wlan0', 'enp0s3', 'ens33', 'lo']
            for interface in common_interfaces:
                if validate_interface(interface):
                    return interface
            return 'eth0'  # é»˜è®¤è¿”å›eth0
    except Exception:
        import platform
        return 'en0' if platform.system().lower() == 'darwin' else 'eth0'

def list_available_interfaces() -> list:
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ç½‘ç»œæ¥å£"""
    try:
        import platform
        system = platform.system().lower()

        if system == 'darwin':  # macOS
            result = subprocess.run(['ifconfig', '-l'], capture_output=True, text=True)
            if result.returncode == 0:
                return result.stdout.strip().split()
        else:  # Linux
            result = subprocess.run(['ip', 'link', 'show'], capture_output=True, text=True)
            if result.returncode == 0:
                interfaces = []
                for line in result.stdout.split('\n'):
                    if ': ' in line and 'state' in line.lower():
                        interface = line.split(':')[1].strip().split('@')[0]
                        interfaces.append(interface)
                return interfaces
        return []
    except Exception:
        return []

def build_tcpdump_command(interface: str, output_file: str, duration: int, filter_expr: str) -> str:
    """æ„å»ºtcpdumpå‘½ä»¤"""
    import platform
    system = platform.system().lower()

    if system == 'darwin':  # macOS
        # macOSä¸‹çš„tcpdumpå‘½ä»¤ï¼Œä½¿ç”¨gtimeoutæˆ–è€…ç›´æ¥ä¾èµ–åç»­çš„subprocess timeout
        cmd_parts = [
            'sudo', 'tcpdump',
            '-i', interface,
            '-w', output_file,
            '-s', '65535',  # æ•è·å®Œæ•´æ•°æ®åŒ…
            '-q'  # å®‰é™æ¨¡å¼ï¼Œå‡å°‘è¾“å‡º
        ]

        if filter_expr:
            # åœ¨macOSä¸‹ï¼Œå°†å¤æ‚çš„è¿‡æ»¤è¡¨è¾¾å¼ç”¨å•å¼•å·åŒ…å›´ä»¥é¿å…shellè§£æé—®é¢˜
            cmd_parts.append(f"'{filter_expr}'")

        # macOSä¸‹ä¸ä½¿ç”¨timeoutå‘½ä»¤ï¼Œè€Œæ˜¯ä¾èµ–subprocessçš„timeoutå‚æ•°
        return ' '.join(cmd_parts)
    else:  # Linux
        cmd_parts = [
            'sudo', 'tcpdump',
            '-i', interface,
            '-w', output_file,
            '-G', str(duration),
            '-W', '1',
            '-s', '65535',  # æ•è·å®Œæ•´æ•°æ®åŒ…
            '-q'  # å®‰é™æ¨¡å¼ï¼Œå‡å°‘è¾“å‡º
        ]

        if filter_expr:
            # åœ¨Linuxä¸‹ä¹Ÿç”¨å•å¼•å·åŒ…å›´è¿‡æ»¤è¡¨è¾¾å¼ï¼Œä¿æŒä¸€è‡´æ€§
            cmd_parts.append(f"'{filter_expr}'")

        return ' '.join(cmd_parts)

def get_task_progress(status: str) -> int:
    """æ ¹æ®ä»»åŠ¡çŠ¶æ€è¿”å›è¿›åº¦ç™¾åˆ†æ¯”"""
    progress_map = {
        'pending': 0,
        'capturing': 25,
        'processing': 50,
        'ai_analyzing': 80,
        'done': 100,
        'error': 0
    }
    return progress_map.get(status, 0)

def preprocess_pcap(pcap_file: str, issue_type: str):
    """é¢„å¤„ç†pcapæ–‡ä»¶ï¼Œç”Ÿæˆç»“æ„åŒ–æ‘˜è¦"""
    try:
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(pcap_file):
            return {
                'error': f"pcapæ–‡ä»¶ä¸å­˜åœ¨: {pcap_file}",
                'analysis_time': datetime.now().isoformat()
            }

        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        file_size = os.path.getsize(pcap_file)
        if file_size == 0:
            return {
                'error': "pcapæ–‡ä»¶ä¸ºç©º",
                'analysis_time': datetime.now().isoformat(),
                'file_size': 0
            }

        # ä½¿ç”¨å¢å¼ºçš„åˆ†ææ–¹æ³•ï¼Œç”Ÿæˆå¯¹AIæœ‰ä»·å€¼çš„æ•°æ®
        logger.info("ä½¿ç”¨å¢å¼ºåˆ†ææ–¹æ³•ï¼Œç”Ÿæˆæ·±åº¦ç½‘ç»œæ´å¯Ÿ")

        # ä½¿ç”¨å¢å¼ºçš„pcapåˆ†æ
        enhanced_analysis = get_enhanced_pcap_analysis(pcap_file, issue_type)

        return {
            'enhanced_analysis': enhanced_analysis,
            'analysis_time': datetime.now().isoformat(),
            'file_size': file_size,
            'parsing_method': 'enhanced_tshark_analysis',
            # ä¿æŒå‘åå…¼å®¹
            'statistics': enhanced_analysis.get('basic_stats', {}),
            'sample_packets': []
        }

    except Exception as e:
        logger.error(f"é¢„å¤„ç†pcapæ–‡ä»¶å¤±è´¥: {str(e)}")
        return {
            'error': f"é¢„å¤„ç†å¤±è´¥: {str(e)}",
            'analysis_time': datetime.now().isoformat()
        }

def get_enhanced_pcap_analysis(pcap_file: str, issue_type: str) -> Dict:
    """å¢å¼ºçš„pcapåˆ†æï¼Œç”Ÿæˆå¯¹AIè¯Šæ–­æœ‰ä»·å€¼çš„æ•°æ®"""
    analysis = {
        'basic_stats': {},
        'network_behavior': {},
        'performance_indicators': {},
        'anomaly_detection': {},
        'issue_specific_insights': {},
        'diagnostic_clues': []
    }

    try:
        import platform
        system = platform.system().lower()

        # æ£€æŸ¥tsharkæ˜¯å¦å¯ç”¨
        tshark_paths = [
            '/opt/homebrew/bin/tshark',
            '/usr/local/bin/tshark',
            '/usr/bin/tshark',
            'tshark'
        ]

        tshark_cmd = None
        for path in tshark_paths:
            try:
                result = subprocess.run([path, '-v'], capture_output=True, text=True, timeout=5)
                if result.returncode == 0:
                    tshark_cmd = path
                    break
            except:
                continue

        if not tshark_cmd:
            logger.warning("tsharkä¸å¯ç”¨ï¼Œä½¿ç”¨åŸºç¡€åˆ†æ")
            return get_basic_file_analysis(pcap_file, issue_type)

        # 1. åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
        analysis['basic_stats'] = get_basic_packet_stats(tshark_cmd, pcap_file)

        # 2. ç½‘ç»œè¡Œä¸ºåˆ†æ
        analysis['network_behavior'] = analyze_network_behavior(tshark_cmd, pcap_file)

        # 3. æ€§èƒ½æŒ‡æ ‡åˆ†æ
        analysis['performance_indicators'] = analyze_performance_metrics(tshark_cmd, pcap_file)

        # 4. å¼‚å¸¸æ£€æµ‹
        analysis['anomaly_detection'] = detect_network_anomalies(tshark_cmd, pcap_file)

        # 5. HTTP/HTTPSæµé‡åˆ†æ
        analysis['http_analysis'] = analyze_http_traffic(tshark_cmd, pcap_file)

        # 6. é—®é¢˜ç‰¹å®šæ´å¯Ÿ
        analysis['issue_specific_insights'] = get_issue_specific_insights(tshark_cmd, pcap_file, issue_type)

        # 7. ç”Ÿæˆè¯Šæ–­çº¿ç´¢
        analysis['diagnostic_clues'] = generate_diagnostic_clues(analysis, issue_type)

        logger.info(f"å¢å¼ºåˆ†æå®Œæˆ: {analysis['basic_stats'].get('total_packets', 0)} ä¸ªåŒ…")

    except Exception as e:
        logger.error(f"å¢å¼ºåˆ†æå¤±è´¥: {str(e)}")
        analysis['error'] = str(e)
        # é™çº§åˆ°åŸºç¡€åˆ†æ
        analysis.update(get_basic_file_analysis(pcap_file, issue_type))

    return analysis

def analyze_interconnection_issues(tshark_cmd: str, pcap_file: str) -> Dict:
    """åˆ†æäº’è”äº’é€šé—®é¢˜"""
    try:
        from app.services.interconnection_analyzer import InterconnectionAnalyzer

        analyzer = InterconnectionAnalyzer()
        result = analyzer.analyze_interconnection(tshark_cmd, pcap_file)

        return {
            'targeted_analysis': result.get('report', {}),
            'relevant_metrics': {
                'local_isp': result.get('local_isp', 'unknown'),
                'cross_isp_connections': result.get('analysis_summary', {}).get('cross_isp_connections', 0),
                'avg_cross_isp_latency': result.get('analysis_summary', {}).get('avg_cross_isp_latency', 0)
            },
            'diagnostic_hints': [
                f"ğŸ¢ æœ¬åœ°ISP: {result.get('local_isp', 'unknown')}",
                f"ğŸ”„ è·¨è¿è¥å•†è¿æ¥: {result.get('analysis_summary', {}).get('cross_isp_connections', 0)}ä¸ª",
                f"â±ï¸ è·¨ç½‘å¹³å‡å»¶è¿Ÿ: {result.get('analysis_summary', {}).get('avg_cross_isp_latency', 0):.1f}ms"
            ]
        }

    except Exception as e:
        logger.error(f"äº’è”äº’é€šåˆ†æå¤±è´¥: {str(e)}")
        return {
            'targeted_analysis': {'error': str(e)},
            'relevant_metrics': {},
            'diagnostic_hints': ['äº’è”äº’é€šåˆ†æå¤±è´¥']
        }

def analyze_game_lag_issues(tshark_cmd: str, pcap_file: str) -> Dict:
    """åˆ†ææ¸¸æˆå¡é¡¿é—®é¢˜"""
    try:
        from app.services.game_traffic_analyzer import GameTrafficAnalyzer

        analyzer = GameTrafficAnalyzer()
        result = analyzer.analyze_game_traffic(tshark_cmd, pcap_file)

        return {
            'targeted_analysis': result.get('diagnosis', {}),
            'relevant_metrics': {
                'game_traffic_detected': result.get('game_traffic_detected', False),
                'total_game_servers': result.get('analysis_summary', {}).get('total_game_servers', 0),
                'china_mobile_servers': result.get('analysis_summary', {}).get('china_mobile_servers', 0),
                'avg_latency': result.get('analysis_summary', {}).get('avg_latency', 0),
                'network_quality': result.get('network_quality', 'unknown')
            },
            'diagnostic_hints': [
                f"ğŸ® æ¸¸æˆæµé‡æ£€æµ‹: {'æ˜¯' if result.get('game_traffic_detected', False) else 'å¦'}",
                f"ğŸ¯ æ¸¸æˆæœåŠ¡å™¨æ•°é‡: {result.get('analysis_summary', {}).get('total_game_servers', 0)}ä¸ª",
                f"ğŸ“± ä¸­å›½ç§»åŠ¨æœåŠ¡å™¨: {result.get('analysis_summary', {}).get('china_mobile_servers', 0)}ä¸ª",
                f"â±ï¸ å¹³å‡å»¶è¿Ÿ: {result.get('analysis_summary', {}).get('avg_latency', 0):.1f}ms",
                f"ğŸ“Š ç½‘ç»œè´¨é‡: {result.get('network_quality', 'unknown')}"
            ]
        }

    except Exception as e:
        logger.error(f"æ¸¸æˆå¡é¡¿åˆ†æå¤±è´¥: {str(e)}")
        return {
            'targeted_analysis': {'error': str(e)},
            'relevant_metrics': {},
            'diagnostic_hints': ['æ¸¸æˆå¡é¡¿åˆ†æå¤±è´¥']
        }

def get_basic_packet_stats(tshark_cmd: str, pcap_file: str) -> Dict:
    """è·å–åŸºç¡€åŒ…ç»Ÿè®¡"""
    stats = {
        'total_packets': 0,
        'protocols': {},
        'time_range': {'start': None, 'end': None, 'duration': 0},
        'packet_sizes': {'min': 0, 'max': 0, 'avg': 0},
        'data_volume': {'total_bytes': 0, 'avg_rate': 0}
    }

    try:
        # è·å–åŒ…çš„åŸºæœ¬ä¿¡æ¯ï¼šæ—¶é—´ã€å¤§å°ã€åè®®
        result = subprocess.run([
            tshark_cmd, '-r', pcap_file, '-T', 'fields',
            '-e', 'frame.time_epoch',
            '-e', 'frame.len',
            '-e', 'frame.protocols'
        ], capture_output=True, text=True, timeout=15)

        if result.returncode == 0:
            lines = result.stdout.strip().split('\n')
            packet_times = []
            packet_sizes = []
            protocols = {}

            for line in lines:
                if line.strip():
                    parts = line.strip().split('\t')
                    if len(parts) >= 3:
                        # æ—¶é—´
                        try:
                            timestamp = float(parts[0])
                            packet_times.append(timestamp)
                        except:
                            pass

                        # å¤§å°
                        try:
                            size = int(parts[1])
                            packet_sizes.append(size)
                        except:
                            pass

                        # åè®®
                        protocol_stack = parts[2].split(':')
                        if protocol_stack:
                            top_protocol = protocol_stack[-1].upper()
                            protocols[top_protocol] = protocols.get(top_protocol, 0) + 1

            stats['total_packets'] = len(packet_times)
            stats['protocols'] = protocols

            if packet_times:
                stats['time_range']['start'] = min(packet_times)
                stats['time_range']['end'] = max(packet_times)
                stats['time_range']['duration'] = max(packet_times) - min(packet_times)

            if packet_sizes:
                stats['packet_sizes']['min'] = min(packet_sizes)
                stats['packet_sizes']['max'] = max(packet_sizes)
                stats['packet_sizes']['avg'] = sum(packet_sizes) / len(packet_sizes)
                stats['data_volume']['total_bytes'] = sum(packet_sizes)
                if stats['time_range']['duration'] > 0:
                    stats['data_volume']['avg_rate'] = sum(packet_sizes) / stats['time_range']['duration']

    except Exception as e:
        logger.debug(f"åŸºç¡€ç»Ÿè®¡å¤±è´¥: {str(e)}")

    return stats

def analyze_network_behavior(tshark_cmd: str, pcap_file: str) -> Dict:
    """åˆ†æç½‘ç»œè¡Œä¸ºæ¨¡å¼ - ç®€åŒ–ç‰ˆï¼Œåªä¿ç•™æ ¸å¿ƒè¿æ¥ä¿¡æ¯"""
    behavior = {
        'connection_summary': {}
    }

    try:
        # åªç»Ÿè®¡åŸºç¡€è¿æ¥ä¿¡æ¯ï¼Œä¸æ”¶é›†æŠ€æœ¯ç»†èŠ‚
        result = subprocess.run([
            tshark_cmd, '-r', pcap_file, '-T', 'fields',
            '-e', 'ip.dst', '-e', 'tcp.dstport'
        ], capture_output=True, text=True, timeout=10)

        if result.returncode == 0:
            unique_destinations = set()

            for line in result.stdout.strip().split('\n'):
                if line.strip():
                    parts = line.strip().split('\t')
                    if len(parts) >= 2:
                        dst_ip, dst_port = parts[:2]
                        if dst_ip and dst_port:
                            unique_destinations.add(f"{dst_ip}:{dst_port}")

            behavior['connection_summary']['unique_destinations'] = len(unique_destinations)

    except Exception as e:
        logger.debug(f"ç½‘ç»œè¡Œä¸ºåˆ†æå¤±è´¥: {str(e)}")

    return behavior

def analyze_http_traffic(tshark_cmd: str, pcap_file: str) -> Dict:
    """åˆ†æHTTP/HTTPSæµé‡ - æç®€ç‰ˆï¼Œåªä¿ç•™æ ¸å¿ƒç½‘ç«™ä¿¡æ¯"""
    http_analysis = {
        'websites_accessed': {},
        'connection_summary': {}
    }

    try:
        # åªè¿›è¡ŒåŸºç¡€çš„HTTPç»Ÿè®¡ï¼Œä¸æ”¶é›†è¯¦ç»†ä¿¡æ¯
        result = subprocess.run([
            tshark_cmd, '-r', pcap_file, '-T', 'fields',
            '-e', 'http.host'
        ], capture_output=True, text=True, timeout=10)

        if result.returncode == 0:
            http_hosts = set()
            for line in result.stdout.strip().split('\n'):
                if line.strip():
                    host = line.strip()
                    if host:
                        http_hosts.add(host)

            http_analysis['basic_summary'] = {
                'http_sites_count': len(http_hosts),
                'has_http_traffic': len(http_hosts) > 0
            }

        # åˆ†æHTTPSç½‘ç«™è®¿é—® - åªä¿ç•™æ ¸å¿ƒä¿¡æ¯
        https_result = subprocess.run([
            tshark_cmd, '-r', pcap_file, '-T', 'fields',
            '-e', 'tls.handshake.extensions_server_name'
        ], capture_output=True, text=True, timeout=10)

        if https_result.returncode == 0:
            websites = {}

            for line in https_result.stdout.strip().split('\n'):
                if line.strip():
                    server_name = line.strip()
                    if server_name:
                        websites[server_name] = websites.get(server_name, 0) + 1

            # åªä¿ç•™è®¿é—®æ¬¡æ•°æœ€å¤šçš„å‰10ä¸ªç½‘ç«™
            http_analysis['websites_accessed'] = dict(sorted(websites.items(), key=lambda x: x[1], reverse=True)[:10])
            http_analysis['connection_summary'] = {
                'total_websites': len(websites),
                'has_https_traffic': len(websites) > 0
            }

    except Exception as e:
        logger.debug(f"HTTPæµé‡åˆ†æå¤±è´¥: {str(e)}")
        http_analysis['error'] = str(e)

    return http_analysis

def analyze_performance_metrics(tshark_cmd: str, pcap_file: str) -> Dict:
    """åˆ†ææ€§èƒ½æŒ‡æ ‡"""
    metrics = {
        'latency_indicators': {},
        'throughput_analysis': {},
        'error_rates': {},
        'quality_metrics': {}
    }

    try:
        # åˆ†æTCP RTTå’Œé‡ä¼ 
        result = subprocess.run([
            tshark_cmd, '-r', pcap_file, '-T', 'fields',
            '-e', 'tcp.analysis.ack_rtt',
            '-e', 'tcp.analysis.retransmission',
            '-e', 'tcp.analysis.duplicate_ack',
            '-e', 'tcp.analysis.fast_retransmission'
        ], capture_output=True, text=True, timeout=15)

        if result.returncode == 0:
            rtt_values = []
            retransmissions = 0
            duplicate_acks = 0
            fast_retrans = 0

            for line in result.stdout.strip().split('\n'):
                if line.strip():
                    parts = line.strip().split('\t')

                    # RTTåˆ†æ
                    if len(parts) > 0 and parts[0]:
                        try:
                            rtt = float(parts[0])
                            rtt_values.append(rtt * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
                        except:
                            pass

                    # é”™è¯¯ç»Ÿè®¡
                    if len(parts) > 1 and parts[1]:
                        retransmissions += 1
                    if len(parts) > 2 and parts[2]:
                        duplicate_acks += 1
                    if len(parts) > 3 and parts[3]:
                        fast_retrans += 1

            if rtt_values:
                metrics['latency_indicators']['avg_rtt_ms'] = sum(rtt_values) / len(rtt_values)
                metrics['latency_indicators']['min_rtt_ms'] = min(rtt_values)
                metrics['latency_indicators']['max_rtt_ms'] = max(rtt_values)
                metrics['latency_indicators']['rtt_samples'] = len(rtt_values)

            metrics['error_rates']['retransmissions'] = retransmissions
            metrics['error_rates']['duplicate_acks'] = duplicate_acks
            metrics['error_rates']['fast_retransmissions'] = fast_retrans

    except Exception as e:
        logger.debug(f"æ€§èƒ½æŒ‡æ ‡åˆ†æå¤±è´¥: {str(e)}")

    return metrics

def detect_network_anomalies(tshark_cmd: str, pcap_file: str) -> Dict:
    """æ£€æµ‹ç½‘ç»œå¼‚å¸¸"""
    anomalies = {
        'suspicious_patterns': [],
        'error_indicators': [],
        'performance_issues': [],
        'security_concerns': []
    }

    try:
        # æ£€æµ‹DNSå¼‚å¸¸
        result = subprocess.run([
            tshark_cmd, '-r', pcap_file, '-T', 'fields',
            '-e', 'dns.qry.name', '-e', 'dns.resp.code', '-e', 'dns.time'
        ], capture_output=True, text=True, timeout=10)

        if result.returncode == 0:
            dns_failures = 0
            slow_dns_queries = 0

            for line in result.stdout.strip().split('\n'):
                if line.strip():
                    parts = line.strip().split('\t')
                    if len(parts) >= 2:
                        # DNSå“åº”ç æ£€æŸ¥
                        if parts[1] and parts[1] != '0':
                            dns_failures += 1

                        # DNSæŸ¥è¯¢æ—¶é—´æ£€æŸ¥
                        if len(parts) > 2 and parts[2]:
                            try:
                                dns_time = float(parts[2])
                                if dns_time > 1.0:  # è¶…è¿‡1ç§’è®¤ä¸ºæ˜¯æ…¢æŸ¥è¯¢
                                    slow_dns_queries += 1
                            except:
                                pass

            if dns_failures > 0:
                anomalies['error_indicators'].append(f"DNSæŸ¥è¯¢å¤±è´¥: {dns_failures} æ¬¡")
            if slow_dns_queries > 0:
                anomalies['performance_issues'].append(f"DNSæ…¢æŸ¥è¯¢: {slow_dns_queries} æ¬¡")

    except Exception as e:
        logger.debug(f"å¼‚å¸¸æ£€æµ‹å¤±è´¥: {str(e)}")

    return anomalies

def get_issue_specific_insights(tshark_cmd: str, pcap_file: str, issue_type: str) -> Dict:
    """è·å–é—®é¢˜ç‰¹å®šçš„æ´å¯Ÿ"""
    insights = {
        'targeted_analysis': {},
        'relevant_metrics': {},
        'diagnostic_hints': []
    }

    try:
        if issue_type == 'website_access':
            insights.update(analyze_http_specific_issues(tshark_cmd, pcap_file))
        elif issue_type == 'interconnection':
            insights.update(analyze_interconnection_issues(tshark_cmd, pcap_file))
        elif issue_type == 'game_lag':
            insights.update(analyze_game_lag_issues(tshark_cmd, pcap_file))
        else:
            insights['targeted_analysis'] = {'note': f'é€šç”¨åˆ†æï¼Œé—®é¢˜ç±»å‹: {issue_type}'}

    except Exception as e:
        logger.debug(f"é—®é¢˜ç‰¹å®šåˆ†æå¤±è´¥: {str(e)}")
        insights['error'] = str(e)

    return insights

def analyze_dns_issues(tshark_cmd: str, pcap_file: str) -> Dict:
    """ä¸“é—¨åˆ†æDNSé—®é¢˜"""
    dns_analysis = {
        'dns_queries': {},
        'dns_servers': {},
        'response_times': {},
        'failure_analysis': {}
    }

    try:
        result = subprocess.run([
            tshark_cmd, '-r', pcap_file, '-T', 'fields',
            '-e', 'dns.qry.name', '-e', 'dns.qry.type', '-e', 'dns.resp.code',
            '-e', 'dns.time', '-e', 'ip.dst', '-e', 'dns.flags.response'
        ], capture_output=True, text=True, timeout=10)

        if result.returncode == 0:
            queries = {}
            servers = {}
            response_times = []
            failures = {}

            for line in result.stdout.strip().split('\n'):
                if line.strip():
                    parts = line.strip().split('\t')
                    if len(parts) >= 4:
                        query_name = parts[0] if parts[0] else 'unknown'
                        query_type = parts[1] if parts[1] else 'A'
                        resp_code = parts[2] if parts[2] else '0'
                        dns_time = parts[3] if parts[3] else '0'
                        server_ip = parts[4] if len(parts) > 4 and parts[4] else 'unknown'
                        is_response = parts[5] if len(parts) > 5 and parts[5] else '0'

                        # ç»Ÿè®¡æŸ¥è¯¢
                        queries[query_name] = queries.get(query_name, 0) + 1

                        # ç»Ÿè®¡DNSæœåŠ¡å™¨
                        if is_response == '0':  # è¿™æ˜¯æŸ¥è¯¢ï¼Œä¸æ˜¯å“åº”
                            servers[server_ip] = servers.get(server_ip, 0) + 1

                        # å“åº”æ—¶é—´
                        try:
                            time_val = float(dns_time)
                            if time_val > 0:
                                response_times.append(time_val * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
                        except:
                            pass

                        # å¤±è´¥åˆ†æ
                        if resp_code != '0':
                            failures[resp_code] = failures.get(resp_code, 0) + 1

            dns_analysis['dns_queries']['top_queries'] = dict(sorted(queries.items(), key=lambda x: x[1], reverse=True)[:10])
            dns_analysis['dns_servers']['servers_used'] = servers

            if response_times:
                dns_analysis['response_times']['avg_ms'] = sum(response_times) / len(response_times)
                dns_analysis['response_times']['max_ms'] = max(response_times)
                dns_analysis['response_times']['samples'] = len(response_times)

            dns_analysis['failure_analysis']['error_codes'] = failures

    except Exception as e:
        logger.debug(f"DNSåˆ†æå¤±è´¥: {str(e)}")

    return dns_analysis

def analyze_slow_connection_issues(tshark_cmd: str, pcap_file: str) -> Dict:
    """åˆ†æè¿æ¥æ…¢çš„é—®é¢˜"""
    slow_analysis = {
        'tcp_handshake': {},
        'window_scaling': {},
        'congestion_control': {},
        'bandwidth_utilization': {}
    }

    try:
        # åˆ†æTCPæ¡æ‰‹æ—¶é—´
        result = subprocess.run([
            tshark_cmd, '-r', pcap_file, '-T', 'fields',
            '-e', 'tcp.connection.syn', '-e', 'tcp.connection.synack',
            '-e', 'tcp.window_size', '-e', 'tcp.analysis.ack_rtt'
        ], capture_output=True, text=True, timeout=10)

        if result.returncode == 0:
            handshake_times = []
            window_sizes = []

            for line in result.stdout.strip().split('\n'):
                if line.strip():
                    parts = line.strip().split('\t')

                    # çª—å£å¤§å°åˆ†æ
                    if len(parts) > 2 and parts[2]:
                        try:
                            window_size = int(parts[2])
                            window_sizes.append(window_size)
                        except:
                            pass

            if window_sizes:
                slow_analysis['window_scaling']['avg_window_size'] = sum(window_sizes) / len(window_sizes)
                slow_analysis['window_scaling']['min_window_size'] = min(window_sizes)
                slow_analysis['window_scaling']['max_window_size'] = max(window_sizes)

    except Exception as e:
        logger.debug(f"æ…¢è¿æ¥åˆ†æå¤±è´¥: {str(e)}")

    return slow_analysis

def analyze_http_specific_issues(tshark_cmd: str, pcap_file: str) -> Dict:
    """ä¸“é—¨åˆ†æHTTP/HTTPSç½‘ç«™è®¿é—®é—®é¢˜ - èšç„¦åŸŸåã€IPã€å“åº”æ—¶å»¶å…³è”"""
    http_issues = {
        'website_performance': {},  # æ ¸å¿ƒï¼šåŸŸå-IP-æ—¶å»¶å…³è”
        'response_summary': {},     # ç®€åŒ–çš„å“åº”ç»Ÿè®¡
        'performance_issues': []    # æ€§èƒ½é—®é¢˜åˆ—è¡¨
    }

    try:
        # åˆ†æHTTPæµé‡ï¼ˆå¦‚æœæœ‰ï¼‰
        website_performance = {}

        # 1. åˆ†ææ˜æ–‡HTTPæµé‡
        http_result = subprocess.run([
            tshark_cmd, '-r', pcap_file, '-T', 'fields',
            '-e', 'http.host',                    # åŸŸå
            '-e', 'ip.dst',                       # ç›®æ ‡IP
            '-e', 'http.response.code',           # å“åº”çŠ¶æ€ç 
            '-e', 'tcp.analysis.ack_rtt',         # TCP RTT
            '-e', 'tcp.analysis.initial_rtt',     # åˆå§‹RTT
            '-e', 'frame.time_relative',          # ç›¸å¯¹æ—¶é—´
            '-e', 'http.request.method',          # HTTPæ–¹æ³•
        ], capture_output=True, text=True, timeout=10)

        if http_result.returncode == 0:
            website_performance.update(process_http_data(http_result.stdout, 'HTTP'))

        # 2. åˆ†æHTTPSæµé‡ï¼ˆé€šè¿‡TLS SNIå’ŒTCPè¿æ¥ï¼‰
        https_result = subprocess.run([
            tshark_cmd, '-r', pcap_file, '-T', 'fields',
            '-e', 'tls.handshake.extensions_server_name',  # HTTPSåŸŸåï¼ˆSNIï¼‰
            '-e', 'ip.dst',                                # ç›®æ ‡IP
            '-e', 'tcp.analysis.ack_rtt',                  # TCP RTT
            '-e', 'tcp.analysis.initial_rtt',              # åˆå§‹RTT
            '-e', 'frame.time_relative',                   # ç›¸å¯¹æ—¶é—´
            '-e', 'tcp.dstport',                           # ç›®æ ‡ç«¯å£
        ], capture_output=True, text=True, timeout=10)

        if https_result.returncode == 0:
            website_performance.update(process_https_data(https_result.stdout, 'HTTPS'))

        # æ•´ç†æœ€ç»ˆç»“æœ
        performance_data = {}
        performance_issues = []

        for site_key, data in website_performance.items():
            if not data.get('has_traffic', False):
                continue

            # è½¬æ¢IPé›†åˆä¸ºåˆ—è¡¨
            ips = list(data['ips'])

            # è®¡ç®—TCP RTTç»Ÿè®¡
            tcp_times = data['tcp_rtts']
            tcp_stats = {}
            if tcp_times:
                tcp_stats = {
                    'avg_ms': round(sum(tcp_times) / len(tcp_times), 1),
                    'min_ms': round(min(tcp_times), 1),
                    'max_ms': round(max(tcp_times), 1),
                    'samples': len(tcp_times)
                }

            # è®¡ç®—é”™è¯¯ç‡ï¼ˆä»…å¯¹HTTPæœ‰æ•ˆï¼‰
            error_rate = 0
            if data.get('total_requests', 0) > 0:
                error_rate = (data.get('error_count', 0) / data['total_requests']) * 100

            # ç»„ç»‡æ ¸å¿ƒæ•°æ®
            performance_data[site_key] = {
                'ips': ips,
                'tcp_rtt': tcp_stats,
                'requests': {
                    'total': data.get('total_requests', 0),
                    'errors': data.get('error_count', 0),
                    'error_rate_percent': round(error_rate, 1),
                    'error_codes': data.get('error_codes', {})
                },
                'protocol': data.get('protocol', 'UNKNOWN'),
                'access_duration_seconds': data.get('duration', None)
            }

            # è¯†åˆ«æ€§èƒ½é—®é¢˜
            if tcp_stats.get('avg_ms', 0) > 100:
                performance_issues.append(f"ğŸ“¡ {site_key}: ç½‘ç»œå»¶è¿Ÿé«˜ (å¹³å‡{tcp_stats['avg_ms']}ms)")
            elif tcp_stats.get('avg_ms', 0) > 50:
                performance_issues.append(f"â±ï¸ {site_key}: ç½‘ç»œå»¶è¿Ÿåé«˜ (å¹³å‡{tcp_stats['avg_ms']}ms)")

            if error_rate > 10:
                performance_issues.append(f"âŒ {site_key}: é”™è¯¯ç‡é«˜ ({error_rate:.1f}%)")

            if len(ips) > 1:
                performance_issues.append(f"ğŸ”„ {site_key}: å¤šIPè®¿é—® ({len(ips)}ä¸ªIP)")

        http_issues['website_performance'] = performance_data
        http_issues['performance_issues'] = performance_issues

        # ç®€åŒ–çš„å“åº”ç»Ÿè®¡
        total_sites = len(performance_data)
        total_requests = sum(data['requests']['total'] for data in performance_data.values())
        total_errors = sum(data['requests']['errors'] for data in performance_data.values())

        http_issues['response_summary'] = {
            'websites_accessed': total_sites,
            'total_requests': total_requests,
            'total_errors': total_errors,
            'overall_error_rate_percent': round((total_errors / total_requests) * 100, 1) if total_requests > 0 else 0
        }



    except Exception as e:
        logger.debug(f"HTTPé—®é¢˜åˆ†æå¤±è´¥: {str(e)}")
        http_issues['error'] = str(e)

    return http_issues

def process_http_data(stdout_data: str, protocol: str) -> Dict:
    """å¤„ç†HTTPæ•°æ®"""
    website_data = {}

    for line in stdout_data.strip().split('\n'):
        if line.strip():
            parts = line.strip().split('\t')
            if len(parts) >= 7:
                host = parts[0] if parts[0] else ''
                dst_ip = parts[1] if parts[1] else ''
                resp_code = parts[2] if parts[2] else ''
                tcp_ack_rtt = parts[3] if parts[3] else ''
                tcp_initial_rtt = parts[4] if parts[4] else ''
                frame_time = parts[5] if parts[5] else ''
                http_method = parts[6] if parts[6] else ''

                if host:  # åªå¤„ç†æœ‰åŸŸåçš„HTTPè¯·æ±‚
                    if host not in website_data:
                        website_data[host] = {
                            'ips': set(),
                            'tcp_rtts': [],
                            'total_requests': 0,
                            'error_count': 0,
                            'error_codes': {},
                            'protocol': protocol,
                            'has_traffic': False
                        }

                    site_data = website_data[host]

                    if dst_ip:
                        site_data['ips'].add(dst_ip)

                    if http_method or resp_code:
                        site_data['total_requests'] += 1
                        site_data['has_traffic'] = True

                    # è®°å½•TCP RTT
                    rtt_recorded = False
                    if tcp_ack_rtt:
                        try:
                            rtt_ms = float(tcp_ack_rtt) * 1000
                            if rtt_ms > 0:
                                site_data['tcp_rtts'].append(rtt_ms)
                                rtt_recorded = True
                        except:
                            pass

                    if not rtt_recorded and tcp_initial_rtt:
                        try:
                            rtt_ms = float(tcp_initial_rtt) * 1000
                            if rtt_ms > 0:
                                site_data['tcp_rtts'].append(rtt_ms)
                        except:
                            pass

                    # è®°å½•HTTPé”™è¯¯
                    if resp_code and (resp_code.startswith('4') or resp_code.startswith('5')):
                        site_data['error_count'] += 1
                        site_data['error_codes'][resp_code] = site_data['error_codes'].get(resp_code, 0) + 1

    return website_data

def process_https_data(stdout_data: str, protocol: str) -> Dict:
    """å¤„ç†HTTPSæ•°æ®"""
    website_data = {}

    for line in stdout_data.strip().split('\n'):
        if line.strip():
            parts = line.strip().split('\t')
            if len(parts) >= 6:
                server_name = parts[0] if parts[0] else ''
                dst_ip = parts[1] if parts[1] else ''
                tcp_ack_rtt = parts[2] if parts[2] else ''
                tcp_initial_rtt = parts[3] if parts[3] else ''
                frame_time = parts[4] if parts[4] else ''
                dst_port = parts[5] if parts[5] else ''

                # åªå¤„ç†HTTPSç«¯å£çš„è¿æ¥
                if server_name and dst_port in ['443', '8443']:
                    if server_name not in website_data:
                        website_data[server_name] = {
                            'ips': set(),
                            'tcp_rtts': [],
                            'total_requests': 0,
                            'error_count': 0,
                            'error_codes': {},
                            'protocol': protocol,
                            'has_traffic': False
                        }

                    site_data = website_data[server_name]

                    if dst_ip:
                        site_data['ips'].add(dst_ip)
                        site_data['has_traffic'] = True  # HTTPSè¿æ¥å°±ç®—æœ‰æµé‡

                    # è®°å½•TCP RTT
                    rtt_recorded = False
                    if tcp_ack_rtt:
                        try:
                            rtt_ms = float(tcp_ack_rtt) * 1000
                            if rtt_ms > 0:
                                site_data['tcp_rtts'].append(rtt_ms)
                                rtt_recorded = True
                        except:
                            pass

                    if not rtt_recorded and tcp_initial_rtt:
                        try:
                            rtt_ms = float(tcp_initial_rtt) * 1000
                            if rtt_ms > 0:
                                site_data['tcp_rtts'].append(rtt_ms)
                        except:
                            pass

    return website_data

def generate_diagnostic_clues(analysis: Dict, issue_type: str) -> List[str]:
    """ç”Ÿæˆè¯Šæ–­çº¿ç´¢"""
    clues = []

    try:
        basic_stats = analysis.get('basic_stats', {})
        performance = analysis.get('performance_indicators', {})
        anomalies = analysis.get('anomaly_detection', {})
        http_analysis = analysis.get('http_analysis', {})

        # åŸºäºåŒ…æ•°é‡çš„çº¿ç´¢
        total_packets = basic_stats.get('total_packets', 0)
        if total_packets == 0:
            clues.append("âš ï¸ æ²¡æœ‰æ•è·åˆ°æ•°æ®åŒ…ï¼Œå¯èƒ½æ˜¯æƒé™é—®é¢˜æˆ–ç½‘ç»œæ¥å£é€‰æ‹©é”™è¯¯")
        elif total_packets < 10:
            clues.append("âš ï¸ æ•è·çš„æ•°æ®åŒ…å¾ˆå°‘ï¼Œå¯èƒ½éœ€è¦æ›´é•¿çš„æŠ“åŒ…æ—¶é—´")

        # åŸºäºåè®®åˆ†å¸ƒçš„çº¿ç´¢
        protocols = basic_stats.get('protocols', {})
        if 'DNS' in protocols and protocols['DNS'] > total_packets * 0.5:
            clues.append("ğŸ” DNSæµé‡å æ¯”å¾ˆé«˜ï¼Œå¯èƒ½å­˜åœ¨DNSè§£æé—®é¢˜")

        # HTTP/HTTPSæµé‡åˆ†æçº¿ç´¢ - ä½¿ç”¨èšç„¦çš„åˆ†æç»“æœ
        if http_analysis:
            # æ£€æŸ¥æ˜¯å¦æœ‰èšç„¦çš„ç½‘ç«™æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ªissue_specific_insightsï¼‰
            issue_insights = analysis.get('issue_specific_insights', {})
            website_performance = issue_insights.get('website_performance', {})

            if website_performance:
                # ä½¿ç”¨èšç„¦çš„ç½‘ç«™æ€§èƒ½æ•°æ®
                site_count = len(website_performance)
                clues.append(f"ğŸŒ åˆ†æäº† {site_count} ä¸ªç½‘ç«™çš„è®¿é—®æ€§èƒ½")

                # æ˜¾ç¤ºå…·ä½“çš„ç½‘ç«™æ€§èƒ½æ•°æ®ï¼ˆæœ€å¤š3ä¸ªï¼‰
                for host, perf_data in list(website_performance.items())[:3]:
                    ips = perf_data.get('ips', [])
                    tcp_time = perf_data.get('tcp_rtt', {})
                    requests_data = perf_data.get('requests', {})

                    # æ„å»ºIPä¿¡æ¯
                    if ips:
                        ip_info = f"IP: {', '.join(ips[:2])}"
                        if len(ips) > 2:
                            ip_info += f" (+{len(ips)-2}ä¸ª)"
                    else:
                        ip_info = "IP: æœªçŸ¥"

                    # æ„å»ºæ—¶å»¶ä¿¡æ¯ï¼ˆä¸»è¦ä½¿ç”¨TCP RTTï¼‰
                    if tcp_time.get('avg_ms'):
                        time_info = f"å»¶è¿Ÿ: {tcp_time['avg_ms']}ms"
                        # æ·»åŠ å»¶è¿Ÿè¯„ä¼°
                        avg_rtt = tcp_time['avg_ms']
                        if avg_rtt > 100:
                            time_info += " (é«˜)"
                        elif avg_rtt > 50:
                            time_info += " (åé«˜)"
                        else:
                            time_info += " (æ­£å¸¸)"
                    else:
                        time_info = "å»¶è¿Ÿ: æœªæµ‹é‡"

                    # æ„å»ºé”™è¯¯ä¿¡æ¯
                    error_rate = requests_data.get('error_rate_percent', 0)
                    if error_rate > 0:
                        error_info = f"é”™è¯¯ç‡: {error_rate}%"
                    else:
                        error_info = "æ— é”™è¯¯"

                    clues.append(f"ğŸ“Š {host}: {ip_info}, {time_info}, {error_info}")

                # æ˜¾ç¤ºæ€§èƒ½é—®é¢˜
                performance_issues = issue_insights.get('performance_issues', [])
                for issue in performance_issues[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªé—®é¢˜
                    clues.append(issue)

            else:
                # é™çº§åˆ°åŸºç¡€HTTPåˆ†æ
                basic_summary = http_analysis.get('basic_summary', {})
                if basic_summary.get('has_http_traffic'):
                    site_count = basic_summary.get('http_sites_count', 0)
                    if site_count > 0:
                        clues.append(f"ğŸŒ æ£€æµ‹åˆ° {site_count} ä¸ªHTTPç½‘ç«™è®¿é—®")
                        clues.append("âš ï¸ è¯¦ç»†æ€§èƒ½æ•°æ®ä¸å¯ç”¨ï¼Œå»ºè®®é‡æ–°æŠ“åŒ…")

            # HTTPSç½‘ç«™è®¿é—®åˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰
            websites_accessed = http_analysis.get('websites_accessed', {})
            connection_summary = http_analysis.get('connection_summary', {})

            if websites_accessed:
                total_sites = connection_summary.get('total_websites', len(websites_accessed))
                clues.append(f"ğŸŒ è®¿é—®äº† {total_sites} ä¸ªHTTPSç½‘ç«™")

                # æ˜¾ç¤ºè®¿é—®æœ€é¢‘ç¹çš„å‰3ä¸ªç½‘ç«™
                top_sites = list(websites_accessed.items())[:3]
                for site, count in top_sites:
                    clues.append(f"  ğŸ“Š {site}: {count} æ¬¡è¿æ¥")

        # åŸºäºæ€§èƒ½æŒ‡æ ‡çš„çº¿ç´¢
        latency = performance.get('latency_indicators', {})
        if latency.get('avg_rtt_ms', 0) > 100:
            clues.append(f"ğŸŒ å¹³å‡RTTè¾ƒé«˜ ({latency['avg_rtt_ms']:.1f}ms)ï¼Œå¯èƒ½å­˜åœ¨ç½‘ç»œå»¶è¿Ÿé—®é¢˜")

        # åŸºäºé”™è¯¯ç‡çš„çº¿ç´¢
        errors = performance.get('error_rates', {})
        retrans = errors.get('retransmissions', 0)
        if retrans > 0:
            clues.append(f"ğŸ“¡ æ£€æµ‹åˆ° {retrans} æ¬¡TCPé‡ä¼ ï¼Œå¯èƒ½å­˜åœ¨ç½‘ç»œè´¨é‡é—®é¢˜")

        # åŸºäºå¼‚å¸¸æ£€æµ‹çš„çº¿ç´¢
        error_indicators = anomalies.get('error_indicators', [])
        for error in error_indicators:
            clues.append(f"âŒ {error}")

        performance_issues = anomalies.get('performance_issues', [])
        for issue in performance_issues:
            clues.append(f"â±ï¸ {issue}")

        # HTTPé—®é¢˜ç‰¹å®šçº¿ç´¢ - èšç„¦åŸŸå-IP-æ—¶å»¶å…³è”
        if issue_type == 'http':
            issue_insights = analysis.get('issue_specific_insights', {})
            website_performance = issue_insights.get('website_performance', {})
            performance_issues = issue_insights.get('performance_issues', [])

            if website_performance:
                site_count = len(website_performance)
                clues.append(f"ğŸŒ åˆ†æäº† {site_count} ä¸ªç½‘ç«™çš„è®¿é—®æ€§èƒ½")

                # æ˜¾ç¤ºå…·ä½“çš„ç½‘ç«™æ€§èƒ½æ•°æ®
                for host, perf_data in list(website_performance.items())[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                    ips = perf_data.get('ips', [])
                    http_time = perf_data.get('http_response_time', {})
                    tcp_time = perf_data.get('tcp_rtt', {})
                    requests_data = perf_data.get('requests', {})

                    ip_info = f"IP: {', '.join(ips[:2])}" if ips else "IP: æœªçŸ¥"
                    if len(ips) > 2:
                        ip_info += f" (+{len(ips)-2}ä¸ª)"

                    time_info = []
                    if http_time.get('avg_ms'):
                        time_info.append(f"HTTP: {http_time['avg_ms']}ms")
                    if tcp_time.get('avg_ms'):
                        time_info.append(f"TCP: {tcp_time['avg_ms']}ms")

                    error_rate = requests_data.get('error_rate_percent', 0)
                    error_info = f"é”™è¯¯ç‡: {error_rate}%" if error_rate > 0 else "æ— é”™è¯¯"

                    clues.append(f"ğŸ“Š {host}: {ip_info}, {', '.join(time_info)}, {error_info}")

            # æ˜¾ç¤ºæ€§èƒ½é—®é¢˜
            for issue in performance_issues[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªé—®é¢˜
                clues.append(issue)

            if not performance_issues and website_performance:
                clues.append("âœ… ç½‘ç«™è®¿é—®æ€§èƒ½æ­£å¸¸ï¼Œæ— æ˜æ˜¾é—®é¢˜")

        # å…¶ä»–é—®é¢˜ç±»å‹çš„çº¿ç´¢
        elif issue_type == 'dns':
            clues.append("ğŸ” å»ºè®®æ£€æŸ¥DNSæœåŠ¡å™¨é…ç½®å’Œå“åº”æ—¶é—´")
        elif issue_type == 'slow':
            clues.append("ğŸŒ å»ºè®®æ£€æŸ¥ç½‘ç»œå¸¦å®½å’ŒTCPçª—å£å¤§å°")
        elif issue_type == 'disconnect':
            clues.append("ğŸ”Œ å»ºè®®æ£€æŸ¥TCPè¿æ¥é‡ç½®å’Œè¶…æ—¶æƒ…å†µ")

        if not clues:
            clues.append("âœ… ä»æŠ“åŒ…æ•°æ®çœ‹ï¼Œç½‘ç»œè¡Œä¸ºåŸºæœ¬æ­£å¸¸")

    except Exception as e:
        logger.debug(f"ç”Ÿæˆè¯Šæ–­çº¿ç´¢å¤±è´¥: {str(e)}")
        clues.append("âš ï¸ è¯Šæ–­åˆ†æè¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸")

    return clues

def get_basic_file_analysis(pcap_file: str, issue_type: str) -> Dict:
    """åŸºç¡€æ–‡ä»¶åˆ†æï¼ˆé™çº§æ–¹æ¡ˆï¼‰"""
    return {
        'basic_stats': {
            'total_packets': 'unknown',
            'file_size': os.path.getsize(pcap_file) if os.path.exists(pcap_file) else 0
        },
        'diagnostic_clues': [
            "âš ï¸ ä½¿ç”¨åŸºç¡€åˆ†ææ¨¡å¼",
            f"ğŸ“ æ–‡ä»¶å¤§å°: {os.path.getsize(pcap_file) if os.path.exists(pcap_file) else 0} bytes"
        ]
    }

def analyze_by_issue_type_simple(pcap_file: str, issue_type: str) -> Dict:
    """ç®€åŒ–çš„é—®é¢˜ç±»å‹åˆ†æï¼Œé¿å…å¤æ‚çš„pysharkæ“ä½œ"""
    analysis = {
        'issue_type': issue_type,
        'analysis_method': 'simplified',
        'note': 'åŸºäºæ–‡ä»¶å¤§å°å’Œé—®é¢˜ç±»å‹çš„åŸºç¡€åˆ†æ'
    }

    try:
        file_size = os.path.getsize(pcap_file) if os.path.exists(pcap_file) else 0

        # åŸºäºæ–‡ä»¶å¤§å°çš„ç®€å•æ¨æ–­
        if file_size == 0:
            analysis['status'] = 'no_data'
            analysis['message'] = 'æœªæ•è·åˆ°æ•°æ®åŒ…'
        elif file_size < 1024:  # å°äº1KB
            analysis['status'] = 'low_activity'
            analysis['message'] = 'ç½‘ç»œæ´»åŠ¨è¾ƒå°‘'
        elif file_size > 1024 * 1024:  # å¤§äº1MB
            analysis['status'] = 'high_activity'
            analysis['message'] = 'ç½‘ç»œæ´»åŠ¨é¢‘ç¹'
        else:
            analysis['status'] = 'normal_activity'
            analysis['message'] = 'ç½‘ç»œæ´»åŠ¨æ­£å¸¸'

        # æ ¹æ®é—®é¢˜ç±»å‹æ·»åŠ ç‰¹å®šä¿¡æ¯
        if issue_type == 'dns':
            analysis['focus'] = 'DNSæŸ¥è¯¢å’Œå“åº”'
        elif issue_type == 'slow':
            analysis['focus'] = 'TCPè¿æ¥å’Œä¼ è¾“æ€§èƒ½'
        elif issue_type == 'disconnect':
            analysis['focus'] = 'TCPè¿æ¥çŠ¶æ€'
        elif issue_type == 'lan':
            analysis['focus'] = 'ARPå’ŒICMPåè®®'
        elif issue_type == 'video':
            analysis['focus'] = 'UDPæµåª’ä½“ä¼ è¾“'

        analysis['file_size_bytes'] = file_size

    except Exception as e:
        logger.error(f"ç®€åŒ–åˆ†æå¤±è´¥: {str(e)}")
        analysis['error'] = str(e)

    return analysis

def analyze_by_issue_type(pcap_file: str, issue_type: str) -> Dict:
    """æ ¹æ®é—®é¢˜ç±»å‹è¿›è¡Œç‰¹å®šåˆ†æï¼ˆä¿ç•™åŸå‡½æ•°ä»¥å…¼å®¹ï¼‰"""
    # å…ˆå°è¯•ç®€åŒ–åˆ†æ
    return analyze_by_issue_type_simple(pcap_file, issue_type)

def analyze_dns_issues(pcap_file: str) -> Dict:
    """åˆ†æDNSç›¸å…³é—®é¢˜"""
    try:
        cap = pyshark.FileCapture(pcap_file, display_filter='dns')

        dns_queries = 0
        dns_responses = 0
        failed_queries = 0
        response_times = []
        slow_queries = []
        query_response_map = {}

        for pkt in cap:
            try:
                if hasattr(pkt, 'dns'):
                    dns_layer = pkt.dns

                    # DNSæŸ¥è¯¢
                    if hasattr(dns_layer, 'qry_name') and dns_layer.qr == '0':
                        dns_queries += 1
                        query_id = dns_layer.id
                        query_time = float(pkt.sniff_timestamp)
                        query_response_map[query_id] = {
                            'query_time': query_time,
                            'query_name': dns_layer.qry_name
                        }

                    # DNSå“åº”
                    elif dns_layer.qr == '1':
                        dns_responses += 1
                        response_id = dns_layer.id
                        response_time = float(pkt.sniff_timestamp)

                        if response_id in query_response_map:
                            query_info = query_response_map[response_id]
                            rtt = (response_time - query_info['query_time']) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
                            response_times.append(rtt)

                            # æ£€æŸ¥æ˜¯å¦ä¸ºæ…¢æŸ¥è¯¢ï¼ˆ>100msï¼‰
                            if rtt > 100:
                                slow_queries.append({
                                    'query_name': query_info['query_name'],
                                    'response_time': rtt,
                                    'response_code': getattr(dns_layer, 'rcode', 'unknown')
                                })

                            # æ£€æŸ¥æ˜¯å¦ä¸ºå¤±è´¥æŸ¥è¯¢
                            if hasattr(dns_layer, 'rcode') and dns_layer.rcode != '0':
                                failed_queries += 1

                            del query_response_map[response_id]

            except Exception as e:
                logger.debug(f"å¤„ç†DNSåŒ…æ—¶å‡ºé”™: {str(e)}")
                continue

        cap.close()

        avg_response_time = sum(response_times) / len(response_times) if response_times else 0

        return {
            'dns_queries': dns_queries,
            'dns_responses': dns_responses,
            'failed_queries': failed_queries,
            'avg_response_time': round(avg_response_time, 2),
            'slow_queries': slow_queries[:10],  # åªè¿”å›å‰10ä¸ªæ…¢æŸ¥è¯¢
            'total_response_times': len(response_times)
        }

    except Exception as e:
        logger.error(f"DNSåˆ†æå¤±è´¥: {str(e)}")
        return {'error': str(e)}

def analyze_performance_issues(pcap_file: str) -> Dict:
    """åˆ†ææ€§èƒ½ç›¸å…³é—®é¢˜"""
    try:
        cap = pyshark.FileCapture(pcap_file, display_filter='tcp')

        tcp_retransmissions = 0
        tcp_resets = 0
        tcp_fins = 0
        connection_times = {}
        high_latency_connections = []
        total_bytes = 0

        for pkt in cap:
            try:
                if hasattr(pkt, 'tcp'):
                    tcp_layer = pkt.tcp

                    # ç»Ÿè®¡TCPé‡ä¼ 
                    if hasattr(tcp_layer, 'analysis_retransmission'):
                        tcp_retransmissions += 1

                    # ç»Ÿè®¡TCPé‡ç½®
                    if hasattr(tcp_layer, 'flags_reset') and tcp_layer.flags_reset == '1':
                        tcp_resets += 1

                    # ç»Ÿè®¡TCP FIN
                    if hasattr(tcp_layer, 'flags_fin') and tcp_layer.flags_fin == '1':
                        tcp_fins += 1

                    # è®¡ç®—è¿æ¥å»¶è¿Ÿï¼ˆSYNåˆ°SYN-ACKï¼‰
                    if hasattr(tcp_layer, 'flags_syn') and tcp_layer.flags_syn == '1':
                        conn_key = f"{pkt.ip.src}:{tcp_layer.srcport}-{pkt.ip.dst}:{tcp_layer.dstport}"
                        if hasattr(tcp_layer, 'flags_ack') and tcp_layer.flags_ack == '1':
                            # SYN-ACKåŒ…
                            if conn_key in connection_times:
                                rtt = (float(pkt.sniff_timestamp) - connection_times[conn_key]) * 1000
                                if rtt > 100:  # é«˜å»¶è¿Ÿè¿æ¥ï¼ˆ>100msï¼‰
                                    high_latency_connections.append({
                                        'connection': conn_key,
                                        'latency': round(rtt, 2)
                                    })
                        else:
                            # SYNåŒ…
                            connection_times[conn_key] = float(pkt.sniff_timestamp)

                    # ç»Ÿè®¡æµé‡
                    if hasattr(pkt, 'length'):
                        total_bytes += int(pkt.length)

            except Exception as e:
                logger.debug(f"å¤„ç†TCPåŒ…æ—¶å‡ºé”™: {str(e)}")
                continue

        cap.close()

        return {
            'tcp_retransmissions': tcp_retransmissions,
            'tcp_resets': tcp_resets,
            'tcp_fins': tcp_fins,
            'high_latency_connections': high_latency_connections[:10],
            'total_bytes': total_bytes,
            'bandwidth_mbps': round(total_bytes * 8 / (1024 * 1024), 2)  # ä¼°ç®—å¸¦å®½
        }

    except Exception as e:
        logger.error(f"æ€§èƒ½åˆ†æå¤±è´¥: {str(e)}")
        return {'error': str(e)}

def analyze_connection_issues(pcap_file: str) -> Dict:
    """åˆ†æè¿æ¥ç›¸å…³é—®é¢˜"""
    try:
        cap = pyshark.FileCapture(pcap_file, display_filter='tcp')

        connection_attempts = 0
        failed_connections = 0
        connection_resets = 0
        successful_connections = 0

        for pkt in cap:
            try:
                if hasattr(pkt, 'tcp'):
                    tcp_layer = pkt.tcp

                    # SYNåŒ… - è¿æ¥å°è¯•
                    if (hasattr(tcp_layer, 'flags_syn') and tcp_layer.flags_syn == '1' and
                        hasattr(tcp_layer, 'flags_ack') and tcp_layer.flags_ack == '0'):
                        connection_attempts += 1

                    # SYN-ACKåŒ… - æˆåŠŸè¿æ¥
                    elif (hasattr(tcp_layer, 'flags_syn') and tcp_layer.flags_syn == '1' and
                          hasattr(tcp_layer, 'flags_ack') and tcp_layer.flags_ack == '1'):
                        successful_connections += 1

                    # RSTåŒ… - è¿æ¥é‡ç½®
                    elif hasattr(tcp_layer, 'flags_reset') and tcp_layer.flags_reset == '1':
                        connection_resets += 1

            except Exception as e:
                logger.debug(f"å¤„ç†è¿æ¥åŒ…æ—¶å‡ºé”™: {str(e)}")
                continue

        cap.close()

        failed_connections = connection_attempts - successful_connections
        success_rate = (successful_connections / connection_attempts * 100) if connection_attempts > 0 else 0

        return {
            'connection_attempts': connection_attempts,
            'successful_connections': successful_connections,
            'failed_connections': max(0, failed_connections),
            'connection_resets': connection_resets,
            'success_rate': round(success_rate, 2)
        }

    except Exception as e:
        logger.error(f"è¿æ¥åˆ†æå¤±è´¥: {str(e)}")
        return {'error': str(e)}

def analyze_lan_issues(pcap_file: str) -> Dict:
    """åˆ†æå±€åŸŸç½‘ç›¸å…³é—®é¢˜"""
    try:
        cap = pyshark.FileCapture(pcap_file, display_filter='arp or icmp')

        arp_requests = 0
        arp_responses = 0
        icmp_packets = 0
        icmp_unreachable = 0
        ping_requests = 0
        ping_responses = 0

        for pkt in cap:
            try:
                # ARPåˆ†æ
                if hasattr(pkt, 'arp'):
                    arp_layer = pkt.arp
                    if hasattr(arp_layer, 'opcode'):
                        if arp_layer.opcode == '1':  # ARPè¯·æ±‚
                            arp_requests += 1
                        elif arp_layer.opcode == '2':  # ARPå“åº”
                            arp_responses += 1

                # ICMPåˆ†æ
                elif hasattr(pkt, 'icmp'):
                    icmp_layer = pkt.icmp
                    icmp_packets += 1

                    if hasattr(icmp_layer, 'type'):
                        if icmp_layer.type == '8':  # Echo Request (ping)
                            ping_requests += 1
                        elif icmp_layer.type == '0':  # Echo Reply
                            ping_responses += 1
                        elif icmp_layer.type == '3':  # Destination Unreachable
                            icmp_unreachable += 1

            except Exception as e:
                logger.debug(f"å¤„ç†LANåŒ…æ—¶å‡ºé”™: {str(e)}")
                continue

        cap.close()

        arp_success_rate = (arp_responses / arp_requests * 100) if arp_requests > 0 else 0
        ping_success_rate = (ping_responses / ping_requests * 100) if ping_requests > 0 else 0

        return {
            'arp_requests': arp_requests,
            'arp_responses': arp_responses,
            'arp_success_rate': round(arp_success_rate, 2),
            'icmp_packets': icmp_packets,
            'ping_requests': ping_requests,
            'ping_responses': ping_responses,
            'ping_success_rate': round(ping_success_rate, 2),
            'icmp_unreachable': icmp_unreachable
        }

    except Exception as e:
        logger.error(f"LANåˆ†æå¤±è´¥: {str(e)}")
        return {'error': str(e)}

def analyze_video_issues(pcap_file: str) -> Dict:
    """åˆ†æè§†é¢‘ç›¸å…³é—®é¢˜"""
    try:
        cap = pyshark.FileCapture(pcap_file, display_filter='udp')

        udp_packets = 0
        total_bytes = 0
        packet_times = []
        packet_sizes = []

        for pkt in cap:
            try:
                if hasattr(pkt, 'udp'):
                    udp_packets += 1

                    if hasattr(pkt, 'length'):
                        packet_size = int(pkt.length)
                        total_bytes += packet_size
                        packet_sizes.append(packet_size)

                    packet_times.append(float(pkt.sniff_timestamp))

            except Exception as e:
                logger.debug(f"å¤„ç†UDPåŒ…æ—¶å‡ºé”™: {str(e)}")
                continue

        cap.close()

        # è®¡ç®—æŠ–åŠ¨ï¼ˆç›¸é‚»åŒ…åˆ°è¾¾æ—¶é—´å·®çš„å˜åŒ–ï¼‰
        jitter = 0
        if len(packet_times) > 2:
            intervals = [packet_times[i+1] - packet_times[i] for i in range(len(packet_times)-1)]
            if len(intervals) > 1:
                avg_interval = sum(intervals) / len(intervals)
                jitter = sum(abs(interval - avg_interval) for interval in intervals) / len(intervals)
                jitter = jitter * 1000  # è½¬æ¢ä¸ºæ¯«ç§’

        # è®¡ç®—å¹³å‡åŒ…å¤§å°
        avg_packet_size = sum(packet_sizes) / len(packet_sizes) if packet_sizes else 0

        return {
            'udp_packets': udp_packets,
            'total_bytes': total_bytes,
            'avg_packet_size': round(avg_packet_size, 2),
            'jitter_ms': round(jitter, 2),
            'bandwidth_kbps': round(total_bytes * 8 / 1024, 2) if packet_times else 0
        }

    except Exception as e:
        logger.error(f"è§†é¢‘åˆ†æå¤±è´¥: {str(e)}")
        return {'error': str(e)}

@router.get("/debug/tasks")
def debug_tasks():
    """è°ƒè¯•ï¼šæŸ¥çœ‹æ‰€æœ‰ä»»åŠ¡çŠ¶æ€"""
    return {
        "total_tasks": len(tasks),
        "task_ids": list(tasks.keys()),
        "tasks_summary": {
            task_id: {
                "status": task.get('status') if task else 'None',
                "has_result": bool(task.get('result')) if task else False,
                "has_capture_summary": bool(task.get('result', {}).get('capture_summary')) if task else False
            } for task_id, task in tasks.items()
        }
    }

@router.get("/debug/ai-config")
def debug_ai_config():
    """è°ƒè¯•ï¼šæŸ¥çœ‹AIé…ç½®çŠ¶æ€"""
    try:
        from app.config.ai_config import get_ai_config
        import os

        ai_config = get_ai_config()

        # è·å–ç¯å¢ƒå˜é‡ï¼ˆéšè—æ•æ„Ÿä¿¡æ¯ï¼‰
        env_vars = {}
        for key in ['AI_PROVIDER', 'OPENROUTER_API_KEY', 'OPENROUTER_BASE_URL', 'OPENROUTER_MODEL']:
            value = os.getenv(key, 'NOT_SET')
            if 'API_KEY' in key and value != 'NOT_SET':
                env_vars[key] = f"{value[:10]}...{value[-4:]}" if len(value) > 14 else "***"
            else:
                env_vars[key] = value

        current_config = ai_config.get_current_config()

        return {
            "current_provider": ai_config.current_provider,
            "environment_variables": env_vars,
            "current_config": {
                "name": current_config.name if current_config else None,
                "base_url": current_config.base_url if current_config else None,
                "model": current_config.model if current_config else None,
                "api_key_set": bool(current_config.api_key) if current_config else False,
                "api_key_preview": f"{current_config.api_key[:10]}...{current_config.api_key[-4:]}" if current_config and current_config.api_key else "NOT_SET"
            } if current_config else None,
            "all_providers": {
                name: {
                    "enabled": provider.enabled,
                    "base_url": provider.base_url,
                    "model": provider.model,
                    "api_key_set": bool(provider.api_key)
                } for name, provider in ai_config.providers.items()
            }
        }

    except Exception as e:
        logger.error(f"AIé…ç½®è°ƒè¯•å¤±è´¥: {str(e)}", exc_info=True)
        return {
            "error": str(e)
        }

@router.post("/test-ai")
async def test_ai_analysis():
    """æµ‹è¯•AIåˆ†æåŠŸèƒ½"""
    try:
        from app.services.ai_analysis_service import get_ai_analysis_service

        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_capture_summary = {
            "statistics": {
                "total_packets": 1000,
                "duration": 30,
                "total_bytes": 500000
            },
            "enhanced_analysis": {
                "network_quality": {
                    "avg_rtt": 45.5,
                    "packet_loss_rate": 0.02,
                    "retransmissions": 5
                },
                "http_analysis": {
                    "total_requests": 50,
                    "unique_domains": 8,
                    "https_ratio": 0.9
                }
            }
        }

        ai_service = get_ai_analysis_service()
        result = await ai_service.analyze_network_issue(
            issue_type="slow",
            capture_summary=test_capture_summary,
            user_description="ç½‘ç»œé€Ÿåº¦å¾ˆæ…¢ï¼Œç½‘é¡µåŠ è½½ç¼“æ…¢"
        )

        return {
            "success": True,
            "test_result": result
        }

    except Exception as e:
        logger.error(f"AIæµ‹è¯•å¤±è´¥: {str(e)}", exc_info=True)
        return {
            "success": False,
            "error": str(e)
        }

@router.post("/analyze-ai")
async def start_ai_analysis(request: dict):
    """å¯åŠ¨AIåˆ†æï¼ˆç”¨äºç½‘ç«™æ€§èƒ½å±•ç¤ºåçš„AIåˆ†æï¼‰"""
    try:
        task_id = request.get('task_id')
        logger.info(f"æ”¶åˆ°AIåˆ†æè¯·æ±‚ï¼Œä»»åŠ¡ID: {task_id}")

        if not task_id:
            raise HTTPException(status_code=400, detail="ç¼ºå°‘ä»»åŠ¡ID")

        if task_id not in tasks:
            logger.error(f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}, å½“å‰ä»»åŠ¡åˆ—è¡¨: {list(tasks.keys())}")
            raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")

        task = tasks[task_id]

        # é¢å¤–æ£€æŸ¥taskæ˜¯å¦ä¸ºNone
        if task is None:
            logger.error(f"ä»»åŠ¡æ•°æ®ä¸ºç©º: {task_id}")
            raise HTTPException(status_code=404, detail="ä»»åŠ¡æ•°æ®å¼‚å¸¸")

        logger.info(f"ä»»åŠ¡çŠ¶æ€: {task.get('status', 'unknown')}")

        # æ£€æŸ¥ä»»åŠ¡çŠ¶æ€
        if task.get('status') != 'done':
            raise HTTPException(status_code=400, detail=f"ä»»åŠ¡å°šæœªå®Œæˆæ•°æ®é¢„å¤„ç†ï¼Œå½“å‰çŠ¶æ€: {task.get('status', 'unknown')}")

        # æ£€æŸ¥æ˜¯å¦å·²æœ‰AIåˆ†æç»“æœ
        result = task.get('result', {})
        if not result:
            logger.info(f"ä»»åŠ¡ç»“æœä¸ºç©º: {task_id}")
        else:
            ai_analysis = result.get('ai_analysis')
            if ai_analysis and isinstance(ai_analysis, dict) and ai_analysis.get('success'):
                logger.info(f"AIåˆ†æå·²å®Œæˆ: {task_id}")
                return {"message": "AIåˆ†æå·²å®Œæˆ", "task_id": task_id}

        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºAIåˆ†æä¸­
        task['status'] = 'ai_analyzing'
        task['progress'] = 80
        logger.info(f"å¼€å§‹AIåˆ†æ: {task_id}")

        # å¼‚æ­¥å¯åŠ¨AIåˆ†æ
        asyncio.create_task(run_ai_analysis_async(task_id))

        return {"message": "AIåˆ†æå·²å¯åŠ¨", "task_id": task_id}

    except HTTPException:
        # é‡æ–°æŠ›å‡ºHTTPå¼‚å¸¸
        raise
    except Exception as e:
        logger.error(f"å¯åŠ¨AIåˆ†æå¤±è´¥: {str(e)}", exc_info=True)

        # å®‰å…¨åœ°æ›´æ–°ä»»åŠ¡çŠ¶æ€
        if task_id in tasks and tasks[task_id] is not None:
            tasks[task_id]['status'] = 'error'
            tasks[task_id]['error'] = f"å¯åŠ¨AIåˆ†æå¤±è´¥: {str(e)}"

        raise HTTPException(status_code=500, detail=f"å¯åŠ¨AIåˆ†æå¤±è´¥: {str(e)}")

async def run_ai_analysis_async(task_id: str):
    """å¼‚æ­¥è¿è¡ŒAIåˆ†æ"""
    task = tasks.get(task_id)
    if not task or task is None:
        logger.error(f"ä»»åŠ¡ä¸å­˜åœ¨æˆ–ä¸ºç©º: {task_id}")
        return

    try:
        logger.info(f"å¼€å§‹AIåˆ†æ: {task_id}")

        # è·å–ç°æœ‰ç»“æœ
        result = task.get('result')
        if not result or not isinstance(result, dict):
            logger.error(f"ä»»åŠ¡ç»“æœä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯: {task_id}, result type: {type(result)}")
            task['status'] = 'error'
            task['error'] = 'ä»»åŠ¡ç»“æœæ•°æ®ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯'
            return

        capture_summary = result.get('capture_summary')
        if not capture_summary or not isinstance(capture_summary, dict):
            logger.error(f"æŠ“åŒ…æ‘˜è¦ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯: {task_id}, summary type: {type(capture_summary)}")
            task['status'] = 'error'
            task['error'] = 'æŠ“åŒ…æ‘˜è¦æ•°æ®ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯'
            return

        logger.info(f"æŠ“åŒ…æ‘˜è¦æ•°æ®å¤§å°: {len(str(capture_summary))} å­—ç¬¦")

        # è¿è¡ŒAIåˆ†æ
        ai_service = get_ai_analysis_service()
        logger.info(f"AIæœåŠ¡å®ä¾‹åˆ›å»ºæˆåŠŸ")

        ai_result = await ai_service.analyze_network_issue(
            issue_type=task.get('issue_type', 'unknown'),
            capture_summary=capture_summary,
            user_description=task.get('user_description', '')
        )

        logger.info(f"AIåˆ†æå®Œæˆï¼Œç»“æœ: {ai_result.get('success', False)}")

        # æ›´æ–°ç»“æœ
        result['ai_analysis'] = ai_result
        task['result'] = result
        task['status'] = 'done'
        task['progress'] = 100

        logger.info(f"AIåˆ†æä»»åŠ¡å®Œæˆ: {task_id}")

    except Exception as e:
        logger.error(f"AIåˆ†æå¤±è´¥: {str(e)}", exc_info=True)

        # å®‰å…¨åœ°æ›´æ–°ä»»åŠ¡çŠ¶æ€
        if task_id in tasks and tasks[task_id] is not None:
            task = tasks[task_id]
            task['status'] = 'error'
            task['error'] = f"AIåˆ†æå¤±è´¥: {str(e)}"

            # æ·»åŠ é”™è¯¯çš„AIåˆ†æç»“æœ
            result = task.get('result')
            if not result or not isinstance(result, dict):
                result = {}
                task['result'] = result

            result['ai_analysis'] = {
                'success': False,
                'error': str(e),
                'timestamp': datetime.now().isoformat(),
                'analysis': {
                    'diagnosis': f'AIåˆ†ææœåŠ¡é”™è¯¯: {str(e)}',
                    'severity': 'unknown',
                    'root_cause': 'æœåŠ¡é…ç½®æˆ–ç½‘ç»œè¿æ¥é—®é¢˜',
                    'key_findings': ['AIåˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯'],
                    'recommendations': ['æ£€æŸ¥AIæœåŠ¡é…ç½®', 'éªŒè¯ç½‘ç»œè¿æ¥', 'æŸ¥çœ‹æœåŠ¡æ—¥å¿—'],
                    'diagnostic_clues': [
                        f'âŒ é”™è¯¯ä¿¡æ¯: {str(e)}',
                        'ğŸ”§ å»ºè®®æ£€æŸ¥AIæœåŠ¡çŠ¶æ€',
                        'ğŸ“‹ æŸ¥çœ‹è¯¦ç»†æ—¥å¿—è·å–æ›´å¤šä¿¡æ¯'
                    ],
                    'technical_details': f'é”™è¯¯è¯¦æƒ…: {str(e)}',
                    'confidence': 0,
                    'next_steps': 'è¯·æ£€æŸ¥ç³»ç»Ÿé…ç½®å¹¶é‡è¯•'
                }
            }
            task['result'] = result